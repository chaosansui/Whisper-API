import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from contextlib import asynccontextmanager
from pydantic import BaseModel
import uvicorn
import logging
import asyncio
import time
import traceback
from typing import Optional, List, Dict
from asr_service import WhisperASR

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s",
)
logger = logging.getLogger(__name__)

class AudioResponse(BaseModel):
    transcriptions: List[Dict[str, str | float]]
    detected_language: str = "unknown"

whisperAsr = None

@asynccontextmanager
async def lifespan(app):

    global whisperAsr
    try:
        logger.info("ğŸš€ åˆå§‹åŒ– WhisperASR...")
        whisperAsr = WhisperASR()
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡ï¼ŒåŠ è½½æ¨¡å‹ä¸­...")
        whisperAsr.load_models() 
        logger.info("âœ… æœåŠ¡å¯åŠ¨å®Œæˆ")
        yield
    except Exception as e:
        logger.error(f"âŒ WhisperASR åˆå§‹åŒ–æˆ–æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        logger.error(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
        raise
    finally:
        # å…³é—­æ—¶æ¸…ç†èµ„æº
        if whisperAsr:
            logger.INFO("ğŸ›‘ æœåŠ¡å…³é—­ï¼Œæ¸…ç†èµ„æº...")
            whisperAsr.clear_gpu_memory()

# åˆ›å»º FastAPI åº”ç”¨ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼‰
app = FastAPI(
    title="Whisper + è¯­éŸ³è¯†åˆ« API",
    description="åŸºäº Whisper å’Œè¯´è¯äººåˆ†å‰²çš„è¯­éŸ³è½¬å½•æœåŠ¡ï¼Œä¼˜åŒ–å®¢æœåœºæ™¯ï¼ˆç²¤è¯­/è‹±è¯­ï¼‰",
    version="2.7.2",
    lifespan=lifespan  # æ­£ç¡®è®¾ç½® lifespan
)

@app.post("/transcribe", response_model=AudioResponse)
async def transcribe_audio(
    file: UploadFile = File(...),
    forced_language: Optional[str] = Query(None, description="å¼ºåˆ¶æŒ‡å®šè¯­è¨€ï¼Œå¦‚ï¼šcantonese, english")
):
    try:
        if not whisperAsr or not whisperAsr.model_loaded:
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        if not file.filename.lower().endswith(('.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac')):
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        
        audio_bytes = await file.read()
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°éŸ³é¢‘: {file.filename}, å¤§å°: {len(audio_bytes)} å­—èŠ‚")
        loop = asyncio.get_event_loop()
        transcriptions, detected_language = await loop.run_in_executor(
            whisperAsr.executor, whisperAsr.process_call_recording, audio_bytes, forced_language
        )
        total_time = time.time() - start_time
        logger.info(f"è¯·æ±‚å¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        
        return AudioResponse(
            transcriptions=transcriptions,
            detected_language=detected_language
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        logger.error(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")

@app.get("/")
async def root():
    status = "è¿è¡Œä¸­" if whisperAsr and whisperAsr.model_loaded else "å¯åŠ¨ä¸­"
    return {
        "message": "Whisper + pyannote è¯­éŸ³è¯†åˆ«æœåŠ¡ - å®¢æœä¼˜åŒ–ç‰ˆ",
        "status": status,
        "supported_languages": ["auto", "cantonese", "english", "chinese"],
        "endpoints": {
            "transcribe": "POST /transcribe - ä¸Šä¼ éŸ³é¢‘è¿›è¡Œè½¬å½•å’Œè¯´è¯äººåˆ†å‰²",
            "health": "GET /health - æœåŠ¡å¥åº·æ£€æŸ¥",
            "INFO": "GET /INFO - æ¨¡å‹ä¿¡æ¯",
            "model_status": "GET /model_status - æ¨¡å‹çŠ¶æ€æ£€æŸ¥"
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy" if whisperAsr and whisperAsr.model_loaded else "unhealthy",
        "model_loaded": whisperAsr.model_loaded if whisperAsr else False,
        "gpu_available": whisperAsr.torch.cuda.is_available() if whisperAsr else False,
        "timestamp": time.time()
    }

@app.get("/INFO")
async def model_INFO():
    if not whisperAsr or not whisperAsr.model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    diarization_status = "å·²åŠ è½½" if whisperAsr.diarization_pipeline is not None else "æœªåŠ è½½"
    
    return {
        "model_name": "Whisper-large-v3-cantonese + pyannote/segmentation-3.0",
        "language": "ç²¤è¯­/è‹±è¯­ï¼Œä¼˜åŒ–å®¢æœåœºæ™¯",
        "special_optimization": "è¯´è¯äººåˆ†å‰²ï¼Œç²¤è¯­/è‹±è¯­æ··åˆï¼Œä½å»¶è¿Ÿåˆ†å‰²",
        "device": str(whisperAsr.device),
        "diarization_model": diarization_status,
        "model_loaded": True,
        "optimizations": ["n_mels=256", "beam_size=10-12", "temperature=0.15", "pyannote åˆ†å‰²", "åŒè¯´è¯äººä¼˜åŒ–"]
    }

@app.get("/model_status")
async def model_status():
    """æ£€æŸ¥æ¨¡å‹çŠ¶æ€"""
    if not whisperAsr:
        return {
            "status": "æœåŠ¡æœªåˆå§‹åŒ–",
            "whisper_model": "æœªåŠ è½½",
            "diarization_model": "æœªåŠ è½½",
            "local_model_path": "æœªçŸ¥"
        }
    
    diarization_status = "å·²åŠ è½½" if whisperAsr.diarization_pipeline is not None else "æœªåŠ è½½"
    local_path = getattr(whisperAsr, 'local_segmentation_path', 'æœªè®¾ç½®')
    
    return {
        "status": "è¿è¡Œä¸­" if whisperAsr.model_loaded else "å¯åŠ¨ä¸­",
        "whisper_model": "å·²åŠ è½½" if whisperAsr.whisper_model is not None else "æœªåŠ è½½",
        "diarization_model": diarization_status,
        "local_model_path": local_path,
        "model_loaded": whisperAsr.model_loaded
    }

if __name__ == "__main__":
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8008,
            timeout_keep_alive=300,
        )
    except Exception as e:
        logger.error(f"âŒ Uvicorn å¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")