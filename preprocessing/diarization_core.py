import torch
import numpy as np
from speechbrain.inference import EncoderClassifier 
# å¯¼å…¥æ–°çš„èšç±»åº“ï¼šAgglomerative Clustering å’Œ Silhouette Score
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
import logging
from typing import List, Dict, Tuple
import os

logger = logging.getLogger(__name__)

class DiarizationCore:
    """è´Ÿè´£æå–è¯´è¯äººåµŒå…¥ã€æ‰§è¡Œèšç±»å’Œåˆ†é…è¯´è¯äººæ ‡ç­¾ã€‚"""
    
    # K å€¼æœç´¢èŒƒå›´ï¼šå¯¹äºå®¢æœåœºæ™¯ï¼ŒK=2 åˆ° K=5 æ˜¯åˆç†çš„èŒƒå›´
    MIN_K_SPEAKERS = 2
    MAX_K_SPEAKERS = 5 
    # æœ€å°ç‰‡æ®µæ—¶é•¿ï¼šä½äºæ­¤å€¼çš„ç‰‡æ®µä¸è¿›è¡ŒåµŒå…¥æå–
    MIN_DURATION_FOR_EMBEDDING = 0.25 

    def __init__(self, speaker_embedding_path: str, sample_rate: int, device: str):
        self.sample_rate = sample_rate
        self.device = device
        self.speaker_embedding_path = speaker_embedding_path
        self.speaker_embedder = None
        self._load_embedder()

    def _load_embedder(self):
        """åŠ è½½ SpeechBrain ECAPA-TDNN æ¨¡å‹ (åœ¨ GPU ä¸Šè¿è¡Œ)"""
        if not os.path.isdir(self.speaker_embedding_path):
             raise ValueError(f"âŒ SpeakerEmbedding æ¨¡å‹ç›®å½•ç¼ºå¤±: {self.speaker_embedding_path}")
        
        spk_run_options = {"device": self.device}
        logger.info(f"ğŸ”„ åŠ è½½ SpeakerEmbedding æ¨¡å‹ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        self.speaker_embedder = EncoderClassifier.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb",
            savedir=self.speaker_embedding_path,
            run_opts=spk_run_options
        )
        self.speaker_embedder.eval()
        self.speaker_embedder.to(self.device)
        logger.info("âœ… SpeakerEmbedding æ¨¡å‹åŠ è½½å®Œæˆ")

    def _extract_embeddings(self, audio_np: np.ndarray, segments: List[Dict]) -> Tuple[List[np.ndarray], List[Dict]]:
        """
        [æµç¨‹ 8: åµŒå…¥æå–]
        ä» VAD ç‰‡æ®µä¸­æå– ECAPA-TDNN åµŒå…¥å‘é‡ã€‚
        """
        embeddings = []
        valid_segments = []
        min_samples = int(self.MIN_DURATION_FOR_EMBEDDING * self.sample_rate)

        for segment in segments:
            start, end = segment["start"], segment["end"]
            
            # ä»éŸ³é¢‘ numpy æ•°ç»„ä¸­è£å‰ªç‰‡æ®µ
            start_sample = int(start * self.sample_rate)
            end_sample = int(end * self.sample_rate)
            audio_segment = audio_np[start_sample:end_sample]
            
            # è¿‡æ»¤å¤ªçŸ­çš„ç‰‡æ®µ (å°äº 0.25s)
            if len(audio_segment) < min_samples: 
                continue 

            audio_segment_tensor = torch.from_numpy(audio_segment).float().to(self.device).unsqueeze(0)
            
            # æå–åµŒå…¥
            with torch.no_grad():
                # å…³é”®ï¼šä½¿ç”¨ normalize=True ç¡®ä¿åµŒå…¥å‘é‡æ˜¯å•ä½å‘é‡
                embedding = self.speaker_embedder.encode_batch(audio_segment_tensor, normalize=True)
            
            embeddings.append(embedding.cpu().numpy().squeeze())
            valid_segments.append(segment)
            
        return embeddings, valid_segments

    def _determine_optimal_k(self, embeddings_np: np.ndarray) -> int:
        """
        ä½¿ç”¨ Agglomerative Clustering å’Œ Silhouette Score åŠ¨æ€ç¡®å®šæœ€ä¼˜ K å€¼ã€‚
        """
        n_samples = len(embeddings_np)
        
        # å¦‚æœæœ‰æ•ˆç‰‡æ®µå°‘äº 2 ä¸ªï¼Œåˆ™åªèƒ½æ˜¯ K=1
        if n_samples < self.MIN_K_SPEAKERS:
             return 1
             
        # K å€¼æœç´¢èŒƒå›´ä¸Šé™
        max_k = min(n_samples - 1, self.MAX_K_SPEAKERS)
        if max_k < self.MIN_K_SPEAKERS:
             return 1 

        best_k = self.MIN_K_SPEAKERS
        best_score = -1.0
        
        logger.info(f"ğŸ” æ­£åœ¨ {self.MIN_K_SPEAKERS} åˆ° {max_k} èŒƒå›´å†…æœç´¢æœ€ä¼˜ K å€¼...")

        # æ³¨æ„ï¼šAgglomerative Clustering éœ€è¦åŸå§‹åµŒå…¥ï¼Œè€Œä¸æ˜¯ç›¸ä¼¼åº¦çŸ©é˜µ
        for k in range(self.MIN_K_SPEAKERS, max_k + 1):
            try:
                clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')
                labels = clustering.fit_predict(embeddings_np)
                
                # è®¡ç®— Silhouette Score (è½®å»“ç³»æ•°)
                score = silhouette_score(embeddings_np, labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
                
                logger.debug(f"  K={k}, Silhouette Score: {score:.4f}")
            except Exception as e:
                # èšç±»å¯èƒ½åœ¨æŸäº›æç«¯æƒ…å†µä¸‹å¤±è´¥
                logger.warning(f"K={k} èšç±»è®¡ç®—å¤±è´¥: {e}")
                continue

        logger.info(f"ğŸ¯ é€‰å®šæœ€ä¼˜ K={best_k} (Score: {best_score:.4f})")
        return best_k


    def _cluster_and_label(self, embeddings: List[np.ndarray], segments: List[Dict]) -> List[Dict]:
        """
        [æµç¨‹ 5: è¯´è¯äººåˆ†å‰²] å’Œ [æµç¨‹ 9: èšç±»åˆå¹¶] çš„èšç±»éƒ¨åˆ†
        æ‰§è¡Œ Agglomerative Clustering å¹¶åˆ†é…è¯´è¯äººæ ‡ç­¾ã€‚
        """
        if len(embeddings) < 2:
            logger.warning("âŒ æå–çš„æœ‰æ•ˆç‰‡æ®µä¸è¶³ 2 ä¸ªï¼Œè·³è¿‡èšç±»ã€‚")
            return [{**s, "speaker": "SPEAKER_00"} for s in segments] 

        logger.info("ğŸ¯ å¼€å§‹è¯´è¯äººèšç±» (Agglomerative Clustering)...")
        
        embeddings_np = np.array(embeddings)
        
        # 1. åŠ¨æ€ç¡®å®š K å€¼
        K_ESTIMATE = self._determine_optimal_k(embeddings_np)
        
        # 2. ä½¿ç”¨ç¡®å®šçš„ K å€¼è¿›è¡Œæœ€ç»ˆèšç±»
        if K_ESTIMATE == 1:
            labels = np.zeros(len(embeddings_np), dtype=int)
        else:
            final_clustering = AgglomerativeClustering(
                n_clusters=K_ESTIMATE, 
                linkage='ward'
            )
            labels = final_clustering.fit_predict(embeddings_np)

        # åˆ†é…è¯´è¯äººæ ‡ç­¾
        for i, segment in enumerate(segments):
            segment["speaker"] = f"SPEAKER_{labels[i]:02d}"
        
        return segments


    def diarize(self, audio_np: np.ndarray, vad_segments: List[Dict]) -> List[Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„ Diarization æµç¨‹ï¼šæå–åµŒå…¥ã€èšç±»ã€åˆ†é…æ ‡ç­¾ã€‚
        """
        if not vad_segments:
            logger.warning("Diadization Core æ¥æ”¶åˆ°ç©ºç‰‡æ®µï¼Œè·³è¿‡å¤„ç†ã€‚")
            return []

        embeddings, valid_segments = self._extract_embeddings(audio_np, vad_segments)
        
        if not valid_segments:
            logger.warning("æ‰€æœ‰ VAD ç‰‡æ®µå‡è¢«è¿‡æ»¤ï¼Œæ— æ³•æå–åµŒå…¥ã€‚")
            return []
            
        labeled_segments = self._cluster_and_label(embeddings, valid_segments)
        
        # 3. åˆå¹¶ç›¸é‚»çš„ç›¸åŒè¯´è¯äººç‰‡æ®µ (æµç¨‹ 9/10 çš„ä¸€éƒ¨åˆ†)
        merged_segments = self._merge_speaker_segments(labeled_segments)
        logger.info(f"ğŸ¯ Diarization Core è¾“å‡º {len(merged_segments)} ä¸ªåˆå¹¶ç‰‡æ®µ")

        return merged_segments
        
    def _merge_speaker_segments(self, segments: List[Dict]) -> List[Dict]:
        """åˆå¹¶ç›¸åŒè¯´è¯äººçš„ç›¸é‚»ç‰‡æ®µ"""
        if not segments:
            return []
        
        merged_segments = []
        current_segment = segments[0]
        
        for next_segment in segments[1:]:
            # è¯´è¯äººç›¸åŒä¸”é—´éš”å°äº 1.5 ç§’åˆ™åˆå¹¶
            if (current_segment['speaker'] == next_segment['speaker'] and 
                next_segment['start'] - current_segment['end'] < 1.5):
                current_segment['end'] = next_segment['end']
            else:
                if current_segment['end'] - current_segment['start'] >= 0.1:
                    merged_segments.append(current_segment)
                current_segment = next_segment
        
        if current_segment['end'] - current_segment['start'] >= 0.1:
            merged_segments.append(current_segment)
        
        return merged_segments