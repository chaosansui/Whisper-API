import torch
import logging
import time
import traceback
import re
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Tuple

from transformers import WhisperProcessor, WhisperForConditionalGeneration
from silero_vad import load_silero_vad, get_speech_timestamps

from config import MODEL_PATH, OPTIMIZATION_CONFIG, DEVICE
from audio_processor import AudioProcessor
from post_processor import PostProcessor

logger = logging.getLogger(__name__)

class WhisperASR:
    """Whisper ASR æ ¸å¿ƒç±» - å®Œå…¨è‡ªåŠ¨è¯­è¨€è¯†åˆ«"""
    
    def __init__(self):
        """åˆå§‹åŒ– WhisperASR"""
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA ä¸å¯ç”¨")

        self.device = DEVICE
        torch.cuda.set_device(0)
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}, GPU: {torch.cuda.get_device_name(0)}")

        self.whisper_model = None
        self.processor = None
        self.vad_model = None
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.model_loaded = False
        
        # åˆå§‹åŒ–å¤„ç†æ¨¡å—
        self.audio_processor = AudioProcessor(sample_rate=OPTIMIZATION_CONFIG["sample_rate"])
        self.post_processor = PostProcessor()
        
        # é…ç½®
        self.optimization_config = OPTIMIZATION_CONFIG
        self.SAMPLE_RATE = OPTIMIZATION_CONFIG["sample_rate"]
        
        self.supported_languages = [
            'English', 'Chinese', 'Cantonese', 'Japanese', 'Korean', 'Vietnamese'
        ]

    def log_memory(self, stage: str):
        """è®°å½• GPU å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == "cuda":
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            free_memory = total_memory - allocated_memory
            logger.info(f"{stage}: å¯ç”¨ {free_memory:.2f} GiB / æ€»å…± {total_memory:.2f} GiB")

    def clear_gpu_memory(self):
        """æ¸…ç† GPU å†…å­˜"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            self.log_memory("æ¸…ç†åå†…å­˜")

    def load_models(self, model_path=MODEL_PATH):
        """åŠ è½½ Whisper å’Œ VAD æ¨¡å‹"""
        try:
            self.clear_gpu_memory()
            logger.info("ğŸ”„ åŠ è½½ Whisper æ¨¡å‹...")
            self.processor = WhisperProcessor.from_pretrained(
                model_path, 
                num_mel_bins=self.optimization_config["n_mels"]
            )
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            ).to(self.device)
            self.whisper_model.eval()
            self.log_memory("Whisper æ¨¡å‹åŠ è½½åå†…å­˜")
            
            logger.info("ğŸ”„ åŠ è½½ Silero VAD æ¨¡å‹...")
            self.vad_model = load_silero_vad(onnx=True)
            self.vad_get_speech_timestamps = get_speech_timestamps
            logger.info("âœ… Silero VAD æ¨¡å‹åŠ è½½å®Œæˆ")
            
            self.model_loaded = True
            self.log_memory("æ‰€æœ‰æ¨¡å‹åŠ è½½åå†…å­˜")
            logger.info("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
            self.warmup_model()
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.model_loaded = False
            raise

    def warmup_model(self):
        """æ¨¡å‹é¢„çƒ­ï¼Œæé«˜é¦–æ¬¡æ¨ç†é€Ÿåº¦"""
        logger.info("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        try:
            t = np.linspace(0, 1, self.SAMPLE_RATE)
            dummy_audio = 0.1 * np.sin(2 * np.pi * 300 * t)
            dummy_audio = dummy_audio.astype(np.float32)
            inputs = self.processor(
                dummy_audio,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt"
            )
            input_features = inputs["input_features"].to(self.device)
            model_dtype = next(self.whisper_model.parameters()).dtype
            input_features = input_features.to(model_dtype)
            with torch.no_grad():
                # é¢„çƒ­å¤šç§è¯­è¨€
                _ = self.whisper_model.generate(
                    input_features,
                    task="transcribe",
                    max_length=10,
                    num_beams=1
                )
            logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}, ç»§ç»­å¯åŠ¨")

    def detect_language(self, audio_np, segment_duration=5.0, retries=3):
        """å®Œå…¨è‡ªåŠ¨è¯­è¨€æ£€æµ‹"""
        try:
            sample_length = min(int(segment_duration * self.SAMPLE_RATE), len(audio_np))
            if sample_length < 8000:  # ç¡®ä¿è‡³å°‘0.5ç§’éŸ³é¢‘
                sample_length = min(8000, len(audio_np))
            sample_audio = audio_np[:sample_length]
            
            inputs = self.processor(
                sample_audio,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt"
            )
            input_features = inputs["input_features"].to(self.device)
            model_dtype = next(self.whisper_model.parameters()).dtype
            input_features = input_features.to(model_dtype)
            
            with torch.no_grad():
                # è®©æ¨¡å‹è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                generated_ids = self.whisper_model.generate(
                    input_features,
                    task="transcribe",
                    max_new_tokens=10,
                    return_dict_in_generate=True,
                    output_scores=True
                )
                
                # ä»ç”Ÿæˆçš„tokenä¸­æå–è¯­è¨€ä¿¡æ¯
                lang_tokens = generated_ids.sequences[0]
                detected_languages = []
                
                for token in lang_tokens:
                    lang_code = self.processor.tokenizer.convert_ids_to_tokens([token])[0]
                    lang_code = re.sub(r'[<>]', '', lang_code).strip().lower()
                    if lang_code in self.supported_languages:
                        detected_languages.append(lang_code)
                
                logger.info(f"è‡ªåŠ¨è¯­è¨€æ£€æµ‹ç»“æœ: {detected_languages}")
                
                # è¿”å›æ£€æµ‹åˆ°çš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆè¯­è¨€
                if detected_languages:
                    detected_lang = detected_languages[0]
                    logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°è¯­è¨€: {detected_lang}")
                    return detected_lang
                
                if retries > 0:
                    logger.warning(f"è¯­è¨€æ£€æµ‹ç»“æœä¸ºç©ºï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {retries}ï¼Œå°è¯•æ›´çŸ­ç‰‡æ®µ")
                    return self.detect_language(audio_np, segment_duration=segment_duration/2, retries=retries-1)
                
                logger.warning("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­è¨€ï¼Œä½¿ç”¨è‡ªåŠ¨æ¨¡å¼")
                return "auto"
                
        except Exception as e:
            logger.warning(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {e}ï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {retries}")
            if retries > 0:
                return self.detect_language(audio_np, segment_duration=segment_duration/2, retries=retries-1)
            logger.warning("è¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨è‡ªåŠ¨æ¨¡å¼")
            return "auto"

    def optimize_vad_parameters(self, audio_tensor):
        """ä¼˜åŒ–VADå‚æ•°"""
        energy = torch.mean(torch.abs(audio_tensor))
        energy_std = torch.std(torch.abs(audio_tensor))
        logger.info(f"éŸ³é¢‘èƒ½é‡æ°´å¹³: {energy:.4f}, èƒ½é‡æ ‡å‡†å·®: {energy_std:.4f}")
        
        if energy < 0.002:
            return {
                "threshold": 0.05,
                "min_silence_duration_ms": 50,
                "min_speech_duration_ms": 50,
                "speech_pad_ms": 10
            }
        elif energy < 0.015:
            return {
                "threshold": 0.02,
                "min_silence_duration_ms": 150,
                "min_speech_duration_ms": 80,
                "speech_pad_ms": 20
            }
        elif energy > 0.08:
            return {
                "threshold": 0.20,
                "min_silence_duration_ms": 100,
                "min_speech_duration_ms": 150,
                "speech_pad_ms": 50
            }
        else:
            return {
                "threshold": 0.05,
                "min_silence_duration_ms": 60,
                "min_speech_duration_ms": 100,
                "speech_pad_ms": 30
            }

    def adaptive_vad_processing(self, audio_tensor):
        """è‡ªé€‚åº”VADå¤„ç†"""
        vad_params = self.optimize_vad_parameters(audio_tensor)
        logger.info(f"ä½¿ç”¨VADå‚æ•°: {vad_params}")
        
        speech_timestamps = self.vad_get_speech_timestamps(
            audio_tensor,
            self.vad_model,
            sampling_rate=self.SAMPLE_RATE,
            **vad_params,
            return_seconds=False
        )
        
        if not speech_timestamps:
            logger.info("å°è¯•æ›´å®½æ¾çš„VADå‚æ•°...")
            fallback_params = [
                {"threshold": 0.008, "min_silence_duration_ms": 30, "min_speech_duration_ms": 50, "speech_pad_ms": 10},
                {"threshold": 0.01, "min_silence_duration_ms": 20, "min_speech_duration_ms": 30, "speech_pad_ms": 5},
                {"threshold": 0.005, "min_silence_duration_ms": 10, "min_speech_duration_ms": 20, "speech_pad_ms": 5},
                {"threshold": 0.001, "min_silence_duration_ms": 10, "min_speech_duration_ms": 10, "speech_pad_ms": 5}
            ]
            
            for params in fallback_params:
                logger.info(f"ä½¿ç”¨å›é€€VADå‚æ•°: {params}")
                speech_timestamps = self.vad_get_speech_timestamps(
                    audio_tensor,
                    self.vad_model,
                    sampling_rate=self.SAMPLE_RATE,
                    **params,
                    return_seconds=False
                )
                if speech_timestamps:
                    break
        
        # å¼ºåˆ¶åˆ†æ®µ
        max_segment_duration = 30 * self.SAMPLE_RATE  # 30ç§’
        final_timestamps = []
        for segment in speech_timestamps:
            start = segment['start']
            end = segment['end']
            segment_duration = end - start
            if segment_duration > max_segment_duration:
                num_splits = int(np.ceil(segment_duration / max_segment_duration))
                split_duration = segment_duration // num_splits
                for i in range(num_splits):
                    split_start = start + i * split_duration
                    split_end = min(split_start + split_duration, end)
                    final_timestamps.append({'start': split_start, 'end': split_end})
            else:
                final_timestamps.append(segment)
        
        logger.info(f"VAD æ£€æµ‹åˆ° {len(final_timestamps)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
        return final_timestamps

    def optimize_generation_parameters(self, audio_duration, audio_quality=0.5, detected_language=None):
        """ä¼˜åŒ–ç”Ÿæˆå‚æ•° - åŸºäºæ£€æµ‹åˆ°çš„è¯­è¨€è‡ªåŠ¨ä¼˜åŒ–"""
        base_params = {
            "max_length": int(audio_duration * 60),
            "num_beams": 12,
            "temperature": 0.2,
            "no_repeat_ngram_size": 3,
            "early_stopping": False,
            "repetition_penalty": 1.2
        }
        
        # åŸºäºæ£€æµ‹åˆ°çš„è¯­è¨€è¿›è¡Œä¼˜åŒ–
        if detected_language == "yue":
            base_params.update({
                "temperature": 0.15,
                "num_beams": 12,
                "repetition_penalty": 1.3,
                "no_repeat_ngram_size": 4
            })
        elif detected_language == "en":
            base_params.update({
                "temperature": 0.15,
                "num_beams": 12,
                "repetition_penalty": 1.25,
                "no_repeat_ngram_size": 3
            })
        elif detected_language == "zh":
            base_params.update({
                "temperature": 0.2,
                "num_beams": 10,
                "repetition_penalty": 1.2
            })
        
        # åŸºäºéŸ³é¢‘æ—¶é•¿è°ƒæ•´
        if audio_duration > 60:
            base_params.update({
                "max_length": int(audio_duration * 80),
                "num_beams": 8
            })
        elif audio_duration < 5:
            base_params.update({
                "max_length": 256,
                "num_beams": 6,
                "early_stopping": True
            })
        
        # åŸºäºéŸ³é¢‘è´¨é‡è°ƒæ•´
        if audio_quality < 0.4:
            base_params.update({
                "num_beams": 14,
                "repetition_penalty": 1.5
            })
            
        return base_params

    def transcribe_segment(self, segment, segment_duration, detected_language="auto", audio_quality=0.5):
        """è½¬å½•å•ä¸ªéŸ³é¢‘ç‰‡æ®µ"""
        try:
            gen_params = self.optimize_generation_parameters(segment_duration, audio_quality, detected_language)
            
            inputs = self.processor(
                segment,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt"
            )
            input_features = inputs["input_features"].to(self.device)
            model_dtype = next(self.whisper_model.parameters()).dtype
            input_features = input_features.to(model_dtype)
            
            with torch.no_grad():
                # è®©æ¨¡å‹è‡ªåŠ¨å¤„ç†è¯­è¨€
                if detected_language != "auto":
                    gen_params["language"] = detected_language
                
                generated_ids = self.whisper_model.generate(
                    input_features,
                    task="transcribe",
                    **gen_params
                )
            
            transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # æ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€è¿›è¡Œåå¤„ç†
            final_transcription = self.clean_text(transcription, detected_language)
            return final_transcription
            
        except Exception as e:
            logger.error(f"ç‰‡æ®µè½¬å½•å¤±è´¥: {e}")
            return ""

    def transcribe_with_vad(self, audio_np, audio_duration, audio_quality=0.5):
        """ä½¿ç”¨VADåˆ†æ®µè½¬å½•"""
        logger.info("ä½¿ç”¨VADåˆ†æ®µå¤„ç†éŸ³é¢‘")
        audio_tensor = torch.from_numpy(audio_np).float().to("cpu")
        if len(audio_tensor.shape) > 1:
            audio_tensor = torch.mean(audio_tensor, dim=1)
            
        speech_timestamps = self.adaptive_vad_processing(audio_tensor)
        all_transcriptions = []
        
        for i, segment_info in enumerate(speech_timestamps):
            start_sample = segment_info['start']
            end_sample = segment_info['end']
            segment = audio_np[start_sample:end_sample]
            segment_duration = (end_sample - start_sample) / self.SAMPLE_RATE
            
            logger.info(f"å¤„ç†ç‰‡æ®µ {i+1}/{len(speech_timestamps)}: æ—¶é•¿: {segment_duration:.2f}ç§’")
            
            # ä¸ºæ¯ä¸ªç‰‡æ®µæ£€æµ‹è¯­è¨€
            segment_language = self.detect_language(segment, segment_duration=min(1.0, segment_duration))
            transcription = self.transcribe_segment(segment, segment_duration, segment_language, audio_quality)
            
            if transcription:
                all_transcriptions.append(transcription)
                logger.info(f"ç‰‡æ®µ {i+1} è½¬å½•ç»“æœ: {transcription} (è¯­è¨€: {segment_language})")
            
            torch.cuda.empty_cache()
        
        final_transcription = " ".join(all_transcriptions)
        return final_transcription, "mixed"

    def transcribe_full_audio(self, audio_np, audio_duration, audio_quality=0.5):
        """ç›´æ¥å¤„ç†å®Œæ•´éŸ³é¢‘"""
        logger.info("ğŸ”„ ç›´æ¥å¤„ç†å®Œæ•´éŸ³é¢‘...")
        
        # æ£€æµ‹æ•´ä½“è¯­è¨€
        detected_language = self.detect_language(audio_np)
        logger.info(f"å®Œæ•´éŸ³é¢‘æ£€æµ‹åˆ°è¯­è¨€: {detected_language}")
        
        # åˆ†å—å¤„ç†é•¿éŸ³é¢‘
        chunk_duration = 30  # æ¯å—30ç§’
        num_chunks = int(np.ceil(audio_duration / chunk_duration))
        all_transcriptions = []
        
        for i in range(num_chunks):
            start_sample = int(i * chunk_duration * self.SAMPLE_RATE)
            end_sample = min(int((i + 1) * chunk_duration * self.SAMPLE_RATE), len(audio_np))
            chunk = audio_np[start_sample:end_sample]
            chunk_duration_actual = (end_sample - start_sample) / self.SAMPLE_RATE
            
            logger.info(f"å¤„ç†éŸ³é¢‘å— {i+1}/{num_chunks}: æ—¶é•¿: {chunk_duration_actual:.2f}ç§’")
            
            transcription = self.transcribe_segment(chunk, chunk_duration_actual, detected_language, audio_quality)
            
            if transcription:
                all_transcriptions.append(transcription)
            
            torch.cuda.empty_cache()
        
        final_transcription = " ".join(all_transcriptions)
        logger.info(f"å®Œæ•´éŸ³é¢‘è½¬å½•ç»“æœ: {final_transcription}")
        return final_transcription, detected_language

    def transcribe_audio(self, audio_bytes, forced_language=None):
        """è½¬å½•å®Œæ•´éŸ³é¢‘ - å®Œå…¨è‡ªåŠ¨è¯­è¨€è¯†åˆ«"""
        if not self.model_loaded:
            logger.error("æ¨¡å‹æœªåŠ è½½")
            return "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€", "unknown"
        
        try:
            start_time = time.time()
            
            # å¤„ç†éŸ³é¢‘
            audio_np = self.process_audio(audio_bytes)
            audio_np = self.enhance_telephone_audio(audio_np)
            audio_duration = len(audio_np) / self.SAMPLE_RATE
            audio_quality = self.estimate_audio_quality(audio_np)
            
            logger.info(f"å¢å¼ºåéŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’, è´¨é‡: {audio_quality:.2f}")
            
            # å®Œå…¨è‡ªåŠ¨è¯­è¨€å¤„ç†
            if audio_duration > 10:  # é•¿éŸ³é¢‘ä½¿ç”¨VADåˆ†æ®µ
                transcription, final_language = self.transcribe_with_vad(audio_np, audio_duration, audio_quality)
            else:  # çŸ­éŸ³é¢‘ç›´æ¥å¤„ç†
                transcription, final_language = self.transcribe_full_audio(audio_np, audio_duration, audio_quality)
            
            logger.info(f"è½¬å½•æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
            return transcription if transcription else "æœªæ£€æµ‹åˆ°è¯­éŸ³", final_language
            
        except Exception as e:
            logger.error(f"è½¬å½•å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return f"è½¬å½•å¤±è´¥: {str(e)}", "error"

    # ä½¿ç”¨æ–°çš„éŸ³é¢‘å¤„ç†å™¨å’Œåå¤„ç†å™¨çš„æ–¹æ³•
    def process_audio(self, audio_bytes):
        """å¤„ç†éŸ³é¢‘æ•°æ® - ä½¿ç”¨æ–°çš„éŸ³é¢‘å¤„ç†å™¨"""
        return self.audio_processor.process_audio(audio_bytes)

    def estimate_audio_quality(self, audio_np):
        """ä¼°è®¡éŸ³é¢‘è´¨é‡ - ä½¿ç”¨æ–°çš„éŸ³é¢‘å¤„ç†å™¨"""
        return self.audio_processor.estimate_audio_quality(audio_np)

    def enhance_telephone_audio(self, audio_np):
        """å¢å¼ºç”µè¯å½•éŸ³è´¨é‡ - ä½¿ç”¨æ–°çš„éŸ³é¢‘å¤„ç†å™¨"""
        return self.audio_processor.enhance_telephone_audio(audio_np)

    def clean_text(self, text, language=None):
        """æ¸…ç†è½¬å½•æ–‡æœ¬ - ä½¿ç”¨æ–°çš„åå¤„ç†å™¨"""
        return self.post_processor.clean_text(text, language)

    def add_custom_correction(self, language: str, wrong: str, correct: str):
        """æ·»åŠ è‡ªå®šä¹‰ä¿®æ­£"""
        self.post_processor.add_custom_correction(language, wrong, correct)

    def batch_add_corrections(self, language: str, corrections: dict):
        """æ‰¹é‡æ·»åŠ ä¿®æ­£"""
        self.post_processor.batch_add_corrections(language, corrections)