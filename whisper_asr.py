import torch
import numpy as np
import time
import logging
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from config import Config
from post_processor import PostProcessor
from audio_processor import AudioProcessor

logger = logging.getLogger(__name__)

class WhisperASR:
    def __init__(self):
        self.device = Config.DEVICE
        self.whisper_model = None
        self.processor = None
        self.post_processor = PostProcessor()
        self.audio_processor = AudioProcessor()
        self.model_loaded = False
        self.context_cache = {}
        
    def load_models(self):
        """ç®€åŒ–æ¨¡å‹åŠ è½½"""
        if self.model_loaded:
            return
            
        try:
            logger.info("ğŸ”„ åŠ è½½ Whisper æ¨¡å‹...")
            
            if self.device.type == "cuda":
                torch.cuda.empty_cache()
                initial_memory = torch.cuda.memory_allocated() / 1024**3
                logger.info(f"ğŸ“Š åŠ è½½å‰æ˜¾å­˜: {initial_memory:.1f}G")
            
            self.processor = WhisperProcessor.from_pretrained(Config.MODEL_PATH)
            
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                Config.MODEL_PATH,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            ).to(self.device)
            
            self.whisper_model.eval()
            
            for param in self.whisper_model.parameters():
                param.requires_grad = False
                
            self.model_loaded = True
            
            if self.device.type == "cuda":
                after_memory = torch.cuda.memory_allocated() / 1024**3
                logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ - æ˜¾å­˜å ç”¨: {after_memory:.1f}G")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise

    def transcribe_audio(self, audio_bytes: bytes, forced_language: str = None, 
                    session_id: str = None, use_context: bool = True) -> tuple:
        """ä¸»è½¬å½•æ¥å£ - ä½¿ç”¨ Silero VAD åˆ†å‰² + ä¿¡ä»»VADæœºåˆ¶"""
        if not self.model_loaded:
            return "æ¨¡å‹æœªåŠ è½½", "error"
            
        try:
            start_time = time.time()
            
            # 1. åŸºç¡€éŸ³é¢‘é¢„å¤„ç†
            logger.info("ğŸ¯ æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç†")
            audio_np = self.audio_processor.preprocess_audio(audio_bytes)
            audio_duration = len(audio_np) / 16000
            logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(audio_np)}æ ·æœ¬, {audio_duration:.2f}ç§’")
            
            # 2. éŸ³é¢‘éªŒè¯ï¼ˆåŸºç¡€æ£€æŸ¥ï¼‰
            if not self.audio_processor.validate_audio(audio_np):
                logger.warning("éŸ³é¢‘è´¨é‡éªŒè¯å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                return "æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³", "auto"
            
            # 3. è·å–ä¸Šä¸‹æ–‡æç¤ºè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            prompt_text = "ç²¤è¯­å®¢æœå¯¹è¯"
            if use_context and session_id and session_id in self.context_cache:
                prompt_text = self.context_cache[session_id]
                logger.info(f"ğŸ“ ä½¿ç”¨ä¸Šä¸‹æ–‡æç¤ºè¯: {prompt_text[:100]}...")
            
            # 4. ä½¿ç”¨ VAD è¿›è¡Œè¯­éŸ³åˆ†å‰²
            logger.info("ğŸ¯ æ­¥éª¤2: VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹")
            speech_segments = self.audio_processor.vad_segmentation(audio_np)
            
            if not speech_segments:
                logger.warning("VAD æœªæ£€æµ‹åˆ°è¯­éŸ³ç‰‡æ®µ")
                return "æœªæ£€æµ‹åˆ°è¯­éŸ³", "auto"
            
            logger.info(f"âœ… VAD æ£€æµ‹åˆ° {len(speech_segments)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            
            # ğŸ¯ è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ªåŸå§‹ç‰‡æ®µçš„é•¿åº¦å’ŒåŸºæœ¬ä¿¡æ¯
            total_speech_duration = 0
            for i, segment in enumerate(speech_segments):
                duration = len(segment) / 16000
                energy = np.mean(np.abs(segment))
                total_speech_duration += duration
                logger.info(f"åŸå§‹ç‰‡æ®µ {i+1}: {duration:.2f}ç§’, {len(segment)}æ ·æœ¬, èƒ½é‡: {energy:.6f}")
            
            speech_ratio = total_speech_duration / audio_duration if audio_duration > 0 else 0
            logger.info(f"è¯­éŸ³ç»Ÿè®¡: æ€»è¯­éŸ³æ—¶é•¿ {total_speech_duration:.2f}ç§’, è¯­éŸ³å æ¯” {speech_ratio:.1%}")
            
            # 5. æ™ºèƒ½åˆå¹¶çŸ­ç‰‡æ®µä»¥æ»¡è¶³ Whisper è¦æ±‚
            merged_segments = self._merge_short_segments(speech_segments, min_duration=15.0)
            logger.info(f"ç‰‡æ®µåˆå¹¶å: {len(speech_segments)} â†’ {len(merged_segments)} ä¸ªç‰‡æ®µ")
            
            # ğŸ¯ ç‰¹æ®Šé€»è¾‘ï¼šå¦‚æœåˆå¹¶åç‰‡æ®µå‡å°‘å¾ˆå¤šï¼Œè¯´æ˜æœ‰å¾ˆå¤šçŸ­è¯­éŸ³
            if len(merged_segments) < len(speech_segments) * 0.5:
                logger.info("ğŸ”Š æ£€æµ‹åˆ°å¤§é‡çŸ­è¯­éŸ³ç‰‡æ®µï¼Œå¯èƒ½æ˜¯é‡å è¯´è¯åœºæ™¯ï¼Œç¡®ä¿å…¨éƒ¨è½¬å½•")
            
            # å¦‚æœåˆå¹¶åè¿˜æ˜¯æ²¡æœ‰æœ‰æ•ˆç‰‡æ®µï¼Œå°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘
            if not merged_segments:
                logger.warning("åˆå¹¶åæ— æœ‰æ•ˆç‰‡æ®µï¼Œå°è¯•ä½¿ç”¨åŸå§‹éŸ³é¢‘")
                merged_segments = [self._pad_to_min_duration(audio_np, 15.0)]
            
            # ğŸ¯ æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ‰€æœ‰ç‰‡æ®µéƒ½æ»¡è¶³Whisperè¦æ±‚
            validated_segments = []
            for i, segment in enumerate(merged_segments):
                segment_duration = len(segment) / 16000
                if segment_duration < 30.0:  # Whisperè¦æ±‚30ç§’
                    logger.info(f"æœ€ç»ˆå¡«å……ç‰‡æ®µ {i+1}: {segment_duration:.2f}ç§’ â†’ 30.00ç§’")
                    segment = self._pad_to_min_duration(segment, 30.0)
                validated_segments.append(segment)
                logger.info(f"âœ… æœ€ç»ˆç‰‡æ®µ {i+1}: {len(segment)/16000:.2f}ç§’")
            
            # 6. è½¬å½•å¤„ç† - ä¿¡ä»»æ‰€æœ‰VADæ£€æµ‹åˆ°çš„ç‰‡æ®µ
            if len(validated_segments) == 1:
                # å•ä¸ªç‰‡æ®µç›´æ¥å¤„ç†
                logger.info("ğŸ“¦ å•ä¸€ç‰‡æ®µï¼Œç›´æ¥è½¬å½•")
                transcription, detected_language = self._transcribe_chunk(
                    validated_segments[0], forced_language, is_first_chunk=True, prompt_text=prompt_text
                )
            else:
                # å¤šä¸ªç‰‡æ®µä½¿ç”¨ä¸Šä¸‹æ–‡ä¼ é€’
                logger.info("ğŸ“¦ å¤šç‰‡æ®µï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡è½¬å½•")
                transcription, detected_language = self._transcribe_vad_segments(
                    validated_segments, forced_language, prompt_text
                )
            
            # 7. æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_context and session_id and transcription and transcription.strip():
                self._update_context_cache(session_id, transcription, detected_language)
            
            total_time = time.time() - start_time
            
            # ğŸ¯ æ€§èƒ½ç»Ÿè®¡
            if transcription and transcription.strip():
                chars_per_second = len(transcription) / total_time if total_time > 0 else 0
                logger.info(f"âœ… è½¬å½•å®Œæˆ - è¯­è¨€: {detected_language}, "
                        f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’, "
                        f"å¤„ç†è€—æ—¶: {total_time:.2f}ç§’, "
                        f"è½¬å½•é€Ÿåº¦: {chars_per_second:.1f}å­—/ç§’")
            else:
                logger.warning(f"âš ï¸ è½¬å½•å®Œæˆä½†æ— æœ‰æ•ˆç»“æœ - è¯­è¨€: {detected_language}, "
                            f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’, "
                            f"å¤„ç†è€—æ—¶: {total_time:.2f}ç§’")
            
            return transcription, detected_language
            
        except Exception as e:
            logger.error(f"âŒ è½¬å½•å¤±è´¥: {str(e)}")
            return f"è½¬å½•å¤±è´¥: {str(e)}", "error"

    def _merge_short_segments(self, segments: list, min_duration: float = 15.0) -> list:
        """æ™ºèƒ½åˆå¹¶çŸ­ç‰‡æ®µä»¥æ»¡è¶³ Whisper è¦æ±‚"""
        if not segments:
            return []
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªç‰‡æ®µï¼Œç›´æ¥æ£€æŸ¥é•¿åº¦
        if len(segments) == 1:
            single_segment = segments[0]
            if len(single_segment) < min_duration * 16000:
                return [self._pad_to_min_duration(single_segment, min_duration)]
            return segments
        
        merged_segments = []
        current_batch = [segments[0]]
        current_total_duration = len(segments[0]) / 16000
        
        for i in range(1, len(segments)):
            segment = segments[i]
            segment_duration = len(segment) / 16000
            
            # å¦‚æœåˆå¹¶åä¸è¶…è¿‡25ç§’ï¼Œå°±ç»§ç»­åˆå¹¶
            if current_total_duration + segment_duration <= 25.0:
                current_batch.append(segment)
                current_total_duration += segment_duration
            else:
                # åˆå¹¶å½“å‰æ‰¹æ¬¡
                if current_batch:
                    merged_segment = self._merge_batch_segments(current_batch)
                    merged_segments.append(merged_segment)
                
                # å¼€å§‹æ–°çš„æ‰¹æ¬¡
                current_batch = [segment]
                current_total_duration = segment_duration
        
        # å¤„ç†æœ€åä¸€æ‰¹
        if current_batch:
            merged_segment = self._merge_batch_segments(current_batch)
            merged_segments.append(merged_segment)
        
        # ç¡®ä¿æ‰€æœ‰ç‰‡æ®µéƒ½æ»¡è¶³æœ€å°é•¿åº¦
        final_segments = []
        for segment in merged_segments:
            if len(segment) < min_duration * 16000:
                padded_segment = self._pad_to_min_duration(segment, min_duration)
                final_segments.append(padded_segment)
            else:
                final_segments.append(segment)
        
        logger.info(f"æ™ºèƒ½åˆå¹¶å®Œæˆ: {len(segments)} â†’ {len(final_segments)} ä¸ªç‰‡æ®µ")
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯ä¸ªæœ€ç»ˆç‰‡æ®µçš„é•¿åº¦
        for i, segment in enumerate(final_segments):
            duration = len(segment) / 16000
            logger.info(f"æœ€ç»ˆç‰‡æ®µ {i+1}: {duration:.2f}ç§’")
        
        return final_segments

    def _merge_batch_segments(self, batch_segments: list) -> np.ndarray:
        """åˆå¹¶ä¸€æ‰¹ç‰‡æ®µï¼Œæ·»åŠ é€‚å½“çš„é™éŸ³é—´éš”"""
        if not batch_segments:
            return np.array([])
        
        if len(batch_segments) == 1:
            return batch_segments[0]
        
        merged_segments = []
        for i, segment in enumerate(batch_segments):
            merged_segments.append(segment)
            # åœ¨ç‰‡æ®µä¹‹é—´æ·»åŠ 0.3ç§’é™éŸ³ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(batch_segments) - 1:
                silence = np.zeros(int(0.3 * 16000))
                merged_segments.append(silence)
        
        return np.concatenate(merged_segments)

    def _pad_to_min_duration(self, audio_np: np.ndarray, min_duration: float) -> np.ndarray:
        """å°†éŸ³é¢‘å¡«å……åˆ°æœ€å°æŒç»­æ—¶é—´"""
        target_samples = int(min_duration * 16000)
        
        if len(audio_np) >= target_samples:
            return audio_np
        
        # è®¡ç®—éœ€è¦å¡«å……çš„é™éŸ³é•¿åº¦
        padding_needed = target_samples - len(audio_np)
        
        # åœ¨å‰åå„å¡«å……ä¸€åŠçš„é™éŸ³
        pad_before = padding_needed // 2
        pad_after = padding_needed - pad_before
        
        padded_audio = np.pad(audio_np, (pad_before, pad_after), mode='constant')
        logger.info(f"éŸ³é¢‘å¡«å……: {len(audio_np)/16000:.2f}ç§’ â†’ {min_duration}ç§’")
        
        return padded_audio

    def _transcribe_vad_segments(self, segments: list, forced_language: str = None, 
                               initial_prompt: str = "") -> tuple:
        """è½¬å½• VAD åˆ†å‰²çš„å¤šä¸ªè¯­éŸ³ç‰‡æ®µ"""
        all_transcriptions = []
        detected_language = "auto"
        current_prompt = initial_prompt
        
        for i, segment in enumerate(segments):
            segment_duration = len(segment) / 16000
            logger.info(f"å¤„ç†è¯­éŸ³ç‰‡æ®µ {i+1}/{len(segments)} (æ—¶é•¿: {segment_duration:.2f}ç§’)")
            
            if current_prompt:
                logger.info(f"ğŸ“ ä½¿ç”¨æç¤ºè¯: {current_prompt[-100:]}...")
            
            segment_transcription, segment_language = self._transcribe_chunk(
                segment, forced_language, 
                is_first_chunk=(i == 0),
                prompt_text=current_prompt
            )
            
            if segment_transcription and segment_transcription.strip():
                all_transcriptions.append(segment_transcription)
                
                # æ›´æ–°æç¤ºè¯ï¼šå°†å½“å‰è¯†åˆ«ç»“æœä½œä¸ºä¸‹ä¸€æ®µçš„æç¤º
                current_prompt = self._truncate_prompt(current_prompt + " " + segment_transcription)
                logger.info(f"âœ… ç‰‡æ®µ {i+1} è½¬å½•æˆåŠŸï¼Œæ›´æ–°æç¤ºè¯")
            else:
                logger.warning(f"âš ï¸ ç‰‡æ®µ {i+1} æ— æœ‰æ•ˆè½¬å½•")
                # å³ä½¿è½¬å½•å¤±è´¥ï¼Œä¹Ÿä¿ç•™ä¹‹å‰çš„æç¤ºè¯
            
            # è®°å½•ç¬¬ä¸€ä¸ªç‰‡æ®µæ£€æµ‹åˆ°çš„è¯­è¨€
            if i == 0 and segment_language and segment_language != "auto":
                detected_language = segment_language
        
        # åˆå¹¶ç»“æœ
        final_transcription = self._merge_transcriptions(all_transcriptions)
        logger.info(f"ğŸ”— åˆå¹¶å®Œæˆ: {len(all_transcriptions)}ä¸ªç‰‡æ®µ -> {len(final_transcription)}å­—ç¬¦")
        
        return final_transcription, detected_language

    def _transcribe_chunk(self, chunk: np.ndarray, forced_language: str = None, 
                         is_first_chunk: bool = False, prompt_text: str = "") -> tuple:
        """è½¬å½•å•ä¸ªéŸ³é¢‘å— - å®Œæ•´ä¿®å¤ç‰ˆ"""
        try:
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨å¼€å§‹æ—¶ä¸¥æ ¼éªŒè¯è¾“å…¥é•¿åº¦
            chunk_duration = len(chunk) / 16000
            logger.info(f"è½¬å½•å¼€å§‹éªŒè¯: è¾“å…¥éŸ³é¢‘{chunk_duration:.2f}ç§’, {len(chunk)}æ ·æœ¬")
            
            # æ ¹æ®å®é™…æµ‹è¯•ï¼ŒWhisper-large-v3 éœ€è¦30ç§’è¾“å…¥
            REQUIRED_DURATION = 30.0
            if chunk_duration < REQUIRED_DURATION:
                logger.warning(f"è¾“å…¥éŸ³é¢‘ä¸è¶³30ç§’: {chunk_duration:.2f}ç§’, å¼ºåˆ¶å¡«å……åˆ°30ç§’")
                chunk = self._pad_to_min_duration(chunk, REQUIRED_DURATION)
                chunk_duration = len(chunk) / 16000
                logger.info(f"å¡«å……å: {chunk_duration:.2f}ç§’")
            
            # ğŸ¯ å…³é”®ä¿®æ”¹ï¼šæå®½æ¾çš„èƒ½é‡æ£€æµ‹ï¼Œåªè¿‡æ»¤å®Œå…¨é™éŸ³
            energy = np.mean(np.abs(chunk))
            logger.info(f"éŸ³é¢‘èƒ½é‡å‚è€ƒå€¼: {energy:.6f}")
            
            # åªè¿‡æ»¤èƒ½é‡æ¥è¿‘0çš„å®Œå…¨é™éŸ³
            if energy < 0.0001:
                logger.warning(f"èƒ½é‡æä½ï¼Œå¯èƒ½ä¸ºè¯¯æ£€é™éŸ³ (èƒ½é‡: {energy:.6f})")
                return "", "auto"
            
            # ğŸ¯ ç‰¹æ®Šæç¤ºï¼šä¸­ç­‰èƒ½é‡å¯èƒ½æ˜¯é‡å è¯´è¯
            if energy < 0.005:
                logger.info(f"æ£€æµ‹åˆ°å¯èƒ½çš„é‡å è¯´è¯æˆ–è½»è¯­éŸ³ (èƒ½é‡: {energy:.6f})ï¼Œç»§ç»­è½¬å½•")

            generation_params = {
                "task": "transcribe",
                "num_beams": 1,
                "temperature": 0.0,
                "no_repeat_ngram_size": 3,
                "compression_ratio_threshold": 2.4,
                "logprob_threshold": -1.0,
                "no_speech_threshold": 0.6,
            }
            
            if forced_language and forced_language != "auto":
                generation_params["language"] = forced_language
            
            # å…¼å®¹çš„æç¤ºè¯å¤„ç†æ–¹å¼
            if prompt_text and prompt_text.strip():
                try:
                    # æ–¹å¼1ï¼šä½¿ç”¨ text å‚æ•°ï¼ˆæ–°ç‰ˆæœ¬ transformersï¼‰
                    inputs = self.processor(
                        chunk,
                        sampling_rate=16000,
                        return_tensors="pt",
                        padding=True,
                        text=prompt_text
                    )
                    logger.debug(f"ä½¿ç”¨æç¤ºè¯(æ–°æ–¹å¼): {prompt_text[:50]}...")
                except Exception as e:
                    logger.warning(f"æ–°æç¤ºè¯æ–¹å¼å¤±è´¥: {e}ï¼Œå°è¯•ä¼ ç»Ÿæ–¹å¼")
                    # æ–¹å¼2ï¼šä¼ ç»Ÿæ–¹å¼
                    inputs = self.processor(
                        chunk,
                        sampling_rate=16000,
                        return_tensors="pt",
                        padding=True
                    )
                    # åœ¨ç”Ÿæˆå‚æ•°ä¸­è®¾ç½®æç¤ºè¯
                    prompt_tokens = self.processor(text=prompt_text, return_tensors="pt").input_ids
                    generation_params["forced_decoder_ids"] = self._create_forced_decoder_ids(prompt_tokens)
            else:
                inputs = self.processor(
                    chunk,
                    sampling_rate=16000,
                    return_tensors="pt",
                    padding=True
                )
            
            input_features = inputs["input_features"].to(self.device, dtype=torch.float16)
            
            with torch.inference_mode():
                generated_ids = self.whisper_model.generate(input_features, **generation_params)
            
            raw_transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # è¿‡æ»¤ç©ºç»“æœ
            if not raw_transcription.strip() or len(raw_transcription.strip()) < 2:
                logger.warning("è½¬å½•ç»“æœä¸ºç©ºæˆ–è¿‡çŸ­")
                return "", "auto"
            
            detected_language = "auto"
            if is_first_chunk and not forced_language:
                detected_language = self._detect_language_from_ids(generated_ids)
            
            # ä½¿ç”¨åå¤„ç†å™¨æ¸…ç†å’ŒçŸ«æ­£æ–‡æœ¬
            transcription = self.post_processor.clean_text(raw_transcription)
            
            # æ¸…ç†å†…å­˜
            del input_features, generated_ids
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info(f"ç‰‡æ®µè½¬å½•ç»“æœ: {transcription[:100]}...")
            return transcription, detected_language
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘ç‰‡æ®µè½¬å½•å¤±è´¥: {e}")
            # å°è¯•ä¸ä½¿ç”¨æç¤ºè¯é‡æ–°è½¬å½•
            if prompt_text:
                logger.info("å°è¯•ä¸ä½¿ç”¨æç¤ºè¯é‡æ–°è½¬å½•...")
                try:
                    return self._transcribe_chunk(chunk, forced_language, is_first_chunk, "")
                except:
                    return "", "auto"
            return "", "auto"

    def _create_forced_decoder_ids(self, prompt_tokens):
        """åˆ›å»ºå¼ºåˆ¶è§£ç å™¨IDï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        forced_decoder_ids = []
        for i, token_id in enumerate(prompt_tokens[0]):
            forced_decoder_ids.append([i + 1, token_id])
        return forced_decoder_ids

    def _truncate_prompt(self, prompt: str, max_tokens: int = 200) -> str:
        """æˆªæ–­æç¤ºè¯ä»¥é¿å…è¿‡é•¿"""
        if not prompt or len(prompt) < 50:
            return prompt
        
        max_chars = max_tokens * 3
        if len(prompt) <= max_chars:
            return prompt
        
        truncated = prompt[-max_chars:]
        sentence_breaks = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼Œ', ',']
        
        for break_char in sentence_breaks:
            break_pos = truncated.find(break_char)
            if break_pos > 10:
                truncated = truncated[break_pos + 1:].lstrip()
                break
        
        logger.debug(f"æç¤ºè¯æˆªæ–­: {len(prompt)} -> {len(truncated)} å­—ç¬¦")
        return truncated

    def _merge_transcriptions(self, transcriptions: list) -> str:
        """åˆå¹¶å¤šä¸ªè½¬å½•ç»“æœ"""
        if not transcriptions:
            return "æœªæ£€æµ‹åˆ°è¯­éŸ³"
        
        merged = " ".join(transcriptions)
        
        # åŸºç¡€å»é‡
        words = merged.split()
        if len(words) > 10:
            for i in range(len(words) - 6):
                if words[i:i+3] == words[i+3:i+6]:
                    merged = " ".join(words[:i+3] + words[i+6:])
                    logger.info("æ£€æµ‹å¹¶ç§»é™¤äº†é‡å¤å†…å®¹")
                    break
        
        return merged

    def _update_context_cache(self, session_id: str, transcription: str, language: str):
        """æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å­˜"""
        max_sessions = 100
        max_length = 800
        
        if len(self.context_cache) >= max_sessions:
            first_key = next(iter(self.context_cache))
            del self.context_cache[first_key]
            logger.info(f"ä¸Šä¸‹æ–‡ç¼“å­˜å·²æ»¡ï¼Œç§»é™¤ä¼šè¯: {first_key}")
        
        truncated_text = transcription[:max_length] if len(transcription) > max_length else transcription
        self.context_cache[session_id] = truncated_text
        logger.info(f"æ›´æ–°ä¼šè¯ {session_id} çš„ä¸Šä¸‹æ–‡ç¼“å­˜: {len(truncated_text)} å­—ç¬¦")

    def clear_context_cache(self, session_id: str = None):
        """æ¸…ç†ä¸Šä¸‹æ–‡ç¼“å­˜"""
        if session_id:
            if session_id in self.context_cache:
                del self.context_cache[session_id]
                logger.info(f"å·²æ¸…ç†ä¼šè¯ {session_id} çš„ä¸Šä¸‹æ–‡ç¼“å­˜")
        else:
            self.context_cache.clear()
            logger.info("å·²æ¸…ç†æ‰€æœ‰ä¸Šä¸‹æ–‡ç¼“å­˜")

    def _detect_language_from_ids(self, generated_ids: torch.Tensor) -> str:
        """ä»ç”Ÿæˆçš„tokenä¸­æ£€æµ‹è¯­è¨€"""
        try:
            if generated_ids.numel() == 0:
                return "auto"
                
            first_tokens = generated_ids[0][:2]
            decoded = self.processor.decode(first_tokens)
            
            if '<|en|>' in decoded:
                return "en"
            elif '<|zh|>' in decoded:
                return "zh" 
            elif '<|yue|>' in decoded:
                return "yue"
            else:
                return "auto"
                
        except Exception as e:
            logger.warning(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
            return "auto"

    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            logger.info("âœ… GPUå†…å­˜å·²æ¸…ç†")

    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.model_loaded:
            return {"status": "æ¨¡å‹æœªåŠ è½½"}
        
        info = {
            "status": "æ¨¡å‹å·²åŠ è½½",
            "model_name": "Whisper-large-v3",
            "device": str(self.device),
            "sample_rate": "16kHz",
            "supported_languages": ["auto", "yue", "zh", "en"],
            "processing_flow": "VADè¯­éŸ³åˆ†å‰² â†’ ç‰‡æ®µåˆå¹¶ â†’ Whisperè½¬å½• â†’ è¯æ±‡çŸ«æ­£",
            "vocabulary_stats": self.get_vocabulary_stats(),
            "context_cache_size": len(self.context_cache),
            "features": [
                "Silero VADè¯­éŸ³æ£€æµ‹",
                "æ™ºèƒ½ç‰‡æ®µåˆå¹¶", 
                "æç¤ºè¯ä¸Šä¸‹æ–‡è¿è´¯æ€§", 
                "è‡ªåŠ¨è¯­è¨€æ£€æµ‹",
                "ç»Ÿä¸€è¯æ±‡çŸ«æ­£"
            ]
        }
        
        if self.device.type == "cuda":
            info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated() / 1024**3:.1f}G"
            
        return info

    # è¯æ±‡å¤„ç†ç›¸å…³æ–¹æ³•ä¿æŒä¸å˜
    def add_custom_correction(self, wrong: str, correct: str):
        """æ·»åŠ è‡ªå®šä¹‰è¯æ±‡çŸ«æ­£"""
        self.post_processor.add_custom_correction(wrong, correct)

    def batch_add_corrections(self, corrections: dict):
        """æ‰¹é‡æ·»åŠ è¯æ±‡çŸ«æ­£"""
        self.post_processor.batch_add_corrections(corrections)

    def remove_correction(self, word: str):
        """ç§»é™¤è¯æ±‡çŸ«æ­£"""
        self.post_processor.remove_correction(word)

    def get_vocabulary_stats(self):
        """è·å–è¯æ±‡è¡¨ç»Ÿè®¡"""
        return self.post_processor.get_vocabulary_stats()

    def search_corrections(self, keyword: str):
        """æœç´¢ç›¸å…³çŸ«æ­£é¡¹"""
        return self.post_processor.search_corrections(keyword)