import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
import io
import soundfile as sf
import numpy as np
import resampy
import traceback
import re
import time
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from contextlib import asynccontextmanager
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from pydantic import BaseModel
import uvicorn
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
from silero_vad import (
    load_silero_vad, get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks
)
import torch.nn as nn
from omegaconf import OmegaConf
from scipy import signal
from scipy.ndimage import uniform_filter1d
from scipy.signal import welch

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# FastAPI é…ç½®
MODEL_PATH = "/mnt/data/models/audio/whisper-large-v3"

# å®šä¹‰å“åº”æ¨¡å‹
class AudioResponse(BaseModel):
    transcription: str
    language: str = "auto"
    detected_language: str = "unknown"

# Whisper ASR ç±»
class WhisperASR:
    def __init__(self):
        """åˆå§‹åŒ– WhisperASR"""
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA ä¸å¯ç”¨")

        self.device = torch.device("cuda:0")
        torch.cuda.set_device(0)
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}, GPU: {torch.cuda.get_device_name(0)}")

        self.whisper_model = None
        self.processor = None
        self.vad_model = None
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.model_loaded = False
        self.SAMPLE_RATE = 16000
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.optimization_config = {
            "use_flash_attention": False,
            "chunk_length": 30,
            "batch_size": 1,
            "max_retries": 5,
            "timeout": 60.0,
            "n_mels": 256 
        }
        
        # æ‰©å±•è¯­è¨€æ˜ å°„è¡¨
        self.language_mapping = {
            "zh": "zh",
            "yue": "yue",
            "yue_Hant": "yue",
            "yue_Hans": "yue",
            "en": "en",
            "cn": "zh",
            "zh-tw": "zh",
            "zh-cn": "zh",
            "ja": "ja",
            "ko": "ko",
            "fr": "fr",
            "de": "de",
            "es": "es",
            "vi": "vi",
            "chinese": "zh",
            "cantonese": "yue",
            "english": "en"
        }
        
        # æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        self.supported_languages = [
            'en', 'zh', 'de', 'es', 'ru', 'ko', 'fr', 'ja', 'pt', 'tr', 'pl', 'ca', 'nl', 'ar', 'sv', 'it',
            'id', 'hi', 'fi', 'vi', 'he', 'uk', 'el', 'ms', 'cs', 'ro', 'da', 'hu', 'ta', 'no', 'th', 'ur',
            'hr', 'bg', 'lt', 'la', 'mi', 'ml', 'cy', 'sk', 'te', 'fa', 'lv', 'bn', 'sr', 'az', 'sl', 'kn',
            'et', 'mk', 'br', 'eu', 'is', 'hy', 'ne', 'mn', 'bs', 'kk', 'sq', 'sw', 'gl', 'mr', 'pa', 'si',
            'km', 'sn', 'yo', 'so', 'af', 'oc', 'ka', 'be', 'tg', 'sd', 'gu', 'am', 'yi', 'lo', 'uz', 'fo',
            'ht', 'ps', 'tk', 'nn', 'mt', 'sa', 'lb', 'my', 'bo', 'tl', 'mg', 'as', 'tt', 'haw', 'ln', 'ha',
            'ba', 'jw', 'su', 'yue'
        ]
        
        # ç²¤è¯­ä¸“ç”¨è¯æ±‡è¡¨
        self.cantonese_corrections = {
            "æ˜¯ä¸æ˜¯": "ä¿‚å””ä¿‚",
            "è¿™æ ·": "å’æ¨£",
            "é‚£ä¸ª": "å—°å€‹",
            "è¿™é‡Œ": "å‘¢åº¦",
            "ä»€ä¹ˆæ—¶å€™": "å¹¾æ™‚",
            "ä¸ºä»€ä¹ˆ": "é»è§£",
            "å¾ˆå¥½": "å¥½å¥½",
            "æ²¡æœ‰": "å†‡",
            "çš„": "å˜…",
            "ä»–": "ä½¢",
            "æˆ‘ä»¬": "æˆ‘å“‹"
        }
        
        # è‹±æ–‡ä¸“ç”¨è¯æ±‡è¡¨
        self.english_corrections = {
            "there": "their",
            "your": "you're",
            "its": "it's",
            "to": "too",
            "then": "than",
            "weather": "whether",
            "accept": "except",
            "OK": "okay",
            "Punky": "Funky",
            "Punky Finance": "Funky Finance"
        }

    def log_memory(self, stage: str):
        """è®°å½• GPU å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == "cuda":
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            free_memory = total_memory - allocated_memory
            logger.info(f"{stage}: å¯ç”¨ {free_memory:.2f} GiB / æ€»å…± {total_memory:.2f} GiB")

    def clear_gpu_memory(self):
        """æ¸…ç† GPU å†…å­˜"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            self.log_memory("æ¸…ç†åå†…å­˜")

    def load_models(self, model_path):
        """åŠ è½½ Whisper å’Œ VAD æ¨¡å‹"""
        try:
            self.clear_gpu_memory()
            logger.info("ğŸ”„ åŠ è½½ Whisper æ¨¡å‹...")
            self.processor = WhisperProcessor.from_pretrained(model_path, num_mel_bins=self.optimization_config["n_mels"])
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            ).to(self.device)
            self.whisper_model.eval()
            self.log_memory("Whisper æ¨¡å‹åŠ è½½åå†…å­˜")
            logger.info("ğŸ”„ åŠ è½½ Silero VAD æ¨¡å‹...")
            self.vad_model = load_silero_vad(onnx=True)
            self.vad_get_speech_timestamps = get_speech_timestamps
            logger.info("âœ… Silero VAD æ¨¡å‹åŠ è½½å®Œæˆ")
            self.model_loaded = True
            self.log_memory("æ‰€æœ‰æ¨¡å‹åŠ è½½åå†…å­˜")
            logger.info("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
            self.warmup_model()
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.model_loaded = False
            raise

    def warmup_model(self):
        """æ¨¡å‹é¢„çƒ­ï¼Œæé«˜é¦–æ¬¡æ¨ç†é€Ÿåº¦"""
        logger.info("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        try:
            t = np.linspace(0, 1, 16000)
            dummy_audio = 0.1 * np.sin(2 * np.pi * 300 * t)
            dummy_audio = dummy_audio.astype(np.float32)
            inputs = self.processor(
                dummy_audio,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt"
            )
            input_features = inputs["input_features"].to(self.device)
            model_dtype = next(self.whisper_model.parameters()).dtype
            input_features = input_features.to(model_dtype)
            with torch.no_grad():
                for lang in ["yue", "en", "zh"]:
                    _ = self.whisper_model.generate(
                        input_features,
                        task="transcribe",
                        max_length=10,
                        num_beams=1,
                        language=lang
                    )
            logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}, ç»§ç»­å¯åŠ¨")

    def detect_language(self, audio_np, segment_duration=5.0, retries=3):
        """ä¼˜åŒ–è¯­è¨€æ£€æµ‹ï¼Œæ”¯æŒç²¤è¯­+è‹±æ–‡æ··åˆ"""
        try:
            sample_length = min(int(segment_duration * self.SAMPLE_RATE), len(audio_np))
            if sample_length < 8000:  # ç¡®ä¿è‡³å°‘0.5ç§’éŸ³é¢‘
                sample_length = min(8000, len(audio_np))
            sample_audio = audio_np[:sample_length]
            inputs = self.processor(
                sample_audio,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt"
            )
            input_features = inputs["input_features"].to(self.device)
            model_dtype = next(self.whisper_model.parameters()).dtype
            input_features = input_features.to(model_dtype)
            with torch.no_grad():
                generated_ids = self.whisper_model.generate(
                    input_features,
                    task="transcribe",
                    max_new_tokens=10,
                    return_dict_in_generate=True
                )
                lang_tokens = generated_ids.sequences[0]
                detected_languages = []
                for token in lang_tokens:
                    lang_code = self.processor.tokenizer.convert_ids_to_tokens([token])[0]
                    lang_code = re.sub(r'[<>]', '', lang_code).strip().lower()
                    if lang_code in self.supported_languages:
                        detected_languages.append((lang_code, 0.5))  # é»˜è®¤æ¦‚ç‡
                logger.info(f"è¯­è¨€æ£€æµ‹ç»“æœ: {detected_languages}")
                for lang_code, prob in detected_languages:
                    if lang_code in ['yue', 'yue_Hant', 'yue_Hans'] and prob > 0.03:
                        logger.info(f"æ£€æµ‹åˆ°ç²¤è¯­ï¼Œæ¦‚ç‡: {prob:.3f}")
                        return self.language_mapping.get(lang_code, lang_code)
                    if lang_code == 'en' and prob > 0.05:
                        logger.info(f"æ£€æµ‹åˆ°è‹±æ–‡ï¼Œæ¦‚ç‡: {prob:.3f}")
                        return self.language_mapping.get(lang_code, lang_code)
                for lang_code, prob in detected_languages:
                    if lang_code in ['zh', 'zh-cn', 'zh-tw', 'cn'] and prob > 0.08:
                        return self.language_mapping.get(lang_code, lang_code)
                if retries > 0:
                    logger.warning(f"è¯­è¨€æ£€æµ‹ç»“æœä¸ºç©ºï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {retries}ï¼Œå°è¯•æ›´çŸ­ç‰‡æ®µ")
                    return self.detect_language(audio_np, segment_duration=segment_duration/2, retries=retries-1)
                logger.warning("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­è¨€ï¼Œå›é€€åˆ°ç²¤è¯­")
                return "yue"
        except Exception as e:
            logger.warning(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {e}ï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {retries}")
            if retries > 0:
                return self.detect_language(audio_np, segment_duration=segment_duration/2, retries=retries-1)
            logger.warning("è¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ç²¤è¯­")
            return "yue"

    def estimate_audio_quality(self, audio_np):
        """ä¼°è®¡éŸ³é¢‘è´¨é‡ï¼Œä¼˜åŒ–ä¿¡å™ªæ¯”è®¡ç®—"""
        try:
            freqs, psd = welch(audio_np, fs=self.SAMPLE_RATE, nperseg=1024)
            signal_power = np.mean(psd[(freqs >= 150) & (freqs <= 4500)])
            noise_power = np.mean(psd[(freqs < 100) | (freqs > 5000)])
            if noise_power > 0:
                snr = 10 * np.log10(signal_power / noise_power)
                quality = min(1.0, max(0.0, (snr - 5) / 30))
            else:
                quality = 0.8
            logger.info(f"éŸ³é¢‘è´¨é‡ä¼°è®¡: SNR={snr:.2f}dB, è´¨é‡={quality:.2f}")
            return quality
        except Exception as e:
            logger.warning(f"éŸ³é¢‘è´¨é‡ä¼°è®¡å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤è´¨é‡ 0.7")
            return 0.7

    def process_audio(self, audio_bytes):
        """å¤„ç†éŸ³é¢‘æ•°æ®"""
        try:
            audio_np, sr = sf.read(io.BytesIO(audio_bytes))
            duration = len(audio_np) / sr
            logger.info(f"åŸå§‹éŸ³é¢‘: å½¢çŠ¶={audio_np.shape}, é‡‡æ ·ç‡={sr}, æ—¶é•¿={duration:.2f}ç§’")
            if len(audio_np.shape) > 1:
                audio_np = np.mean(audio_np, axis=1)
                logger.info(f"é€šé“å¹³å‡å: {audio_np.shape}")
            audio_np = audio_np.astype(np.float32)
            if sr != self.SAMPLE_RATE:
                audio_np = resampy.resample(audio_np, sr, self.SAMPLE_RATE)
                logger.info(f"é‡é‡‡æ ·å: {audio_np.shape}, é‡‡æ ·ç‡={self.SAMPLE_RATE}")
            return audio_np
        except Exception as e:
            logger.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
            raise ValueError(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")

    def enhance_telephone_audio(self, audio_np):
        """å¢å¼ºç”µè¯å½•éŸ³è´¨é‡ï¼Œä¼˜åŒ–ç²¤è¯­å’Œè‹±æ–‡ï¼Œæ·»åŠ é™å™ª"""
        try:
            # åŠ¨æ€å…‰è°±å‡æ³•é™å™ª
            freqs, psd = welch(audio_np, fs=self.SAMPLE_RATE, nperseg=1024)
            noise_psd = np.mean(psd[(freqs < 100) | (freqs > 5000)])
            noise_threshold = np.sqrt(noise_psd) * 0.9
            audio_np[np.abs(audio_np) < noise_threshold] *= 0.1

            # å¸¦é€šæ»¤æ³¢
            nyquist = self.SAMPLE_RATE / 2
            lowcut = 100 / nyquist
            highcut = 5000 / nyquist
            b, a = signal.cheby2(8, 60, [lowcut, highcut], btype='bandpass')
            audio_np = signal.filtfilt(b, a, audio_np)
            
            # è‡ªé€‚åº”å¢ç›Šæ§åˆ¶
            max_val = np.max(np.abs(audio_np))
            if max_val > 0:
                audio_np = audio_np / max_val
                compressed = np.zeros_like(audio_np)
                low_mask = np.abs(audio_np) < 0.06
                compressed[low_mask] = audio_np[low_mask] * 3.0
                mid_mask = (np.abs(audio_np) >= 0.06) & (np.abs(audio_np) < 0.3)
                compressed[mid_mask] = audio_np[mid_mask] * 1.8
                high_mask = np.abs(audio_np) >= 0.3
                compressed[high_mask] = audio_np[high_mask] * 0.7
                audio_np = compressed
            
            # è¯­éŸ³å¢å¼º
            b_eq, a_eq = signal.butter(4, [600/nyquist, 3200/nyquist], btype='bandpass')
            boosted = signal.lfilter(b_eq, a_eq, audio_np)
            audio_np = audio_np * 0.5 + boosted * 0.5
            
            # å¹³æ»‘å¤„ç†
            audio_np = uniform_filter1d(audio_np, size=5)
            logger.info("âœ… é«˜çº§ç”µè¯å½•éŸ³å¢å¼ºå®Œæˆ")
            return audio_np.astype(np.float32)
        except Exception as e:
            logger.warning(f"é«˜çº§éŸ³é¢‘å¢å¼ºå¤±è´¥: {e}, ä½¿ç”¨åŸºç¡€å¢å¼º")
            return self.basic_enhance_telephone_audio(audio_np)

    def basic_enhance_telephone_audio(self, audio_np):
        """åŸºç¡€ç”µè¯å½•éŸ³å¢å¼º"""
        nyquist = self.SAMPLE_RATE / 2
        lowcut = 100 / nyquist
        highcut = 5000 / nyquist
        b, a = signal.butter(4, [lowcut, highcut], btype='band')
        audio_np = signal.filtfilt(b, a, audio_np)
        max_val = np.max(np.abs(audio_np))
        if max_val > 0:
            audio_np = audio_np / max_val * 0.9
        return audio_np

    def optimize_vad_parameters(self, audio_tensor):
        """ä¼˜åŒ–VADå‚æ•°ï¼Œé’ˆå¯¹ç²¤è¯­å’Œè‹±æ–‡"""
        energy = torch.mean(torch.abs(audio_tensor))
        energy_std = torch.std(torch.abs(audio_tensor))
        logger.info(f"éŸ³é¢‘èƒ½é‡æ°´å¹³: {energy:.4f}, èƒ½é‡æ ‡å‡†å·®: {energy_std:.4f}")
        if energy < 0.002:
            return {
                "threshold": 0.05,
                "min_silence_duration_ms": 50,
                "min_speech_duration_ms": 50,
                "speech_pad_ms": 10
            }
        elif energy < 0.015:
            return {
                "threshold": 0.02,
                "min_silence_duration_ms": 150,
                "min_speech_duration_ms": 80,
                "speech_pad_ms": 20
            }
        elif energy > 0.08:
            return {
                "threshold": 0.20,
                "min_silence_duration_ms": 100,
                "min_speech_duration_ms": 150,
                "speech_pad_ms": 50
            }
        else:
            return {
                "threshold": 0.05,
                "min_silence_duration_ms": 60,
                "min_speech_duration_ms": 100,
                "speech_pad_ms": 30
            }

    def adaptive_vad_processing(self, audio_tensor):
        """è‡ªé€‚åº”VADå¤„ç†ï¼Œå¼ºåˆ¶åˆ†æ®µ"""
        vad_params = self.optimize_vad_parameters(audio_tensor)
        logger.info(f"ä½¿ç”¨VADå‚æ•°: {vad_params}")
        speech_timestamps = self.vad_get_speech_timestamps(
            audio_tensor,
            self.vad_model,
            sampling_rate=self.SAMPLE_RATE,
            **vad_params,
            return_seconds=False
        )
        if not speech_timestamps:
            logger.info("å°è¯•æ›´å®½æ¾çš„VADå‚æ•°...")
            fallback_params = [
                {
                    "threshold": 0.008,
                    "min_silence_duration_ms": 30,
                    "min_speech_duration_ms": 50,
                    "speech_pad_ms": 10
                },
                {
                    "threshold": 0.01,
                    "min_silence_duration_ms": 20,
                    "min_speech_duration_ms": 30,
                    "speech_pad_ms": 5
                },
                {
                    "threshold": 0.005,
                    "min_silence_duration_ms": 10,
                    "min_speech_duration_ms": 20,
                    "speech_pad_ms": 5
                },
                {
                    "threshold": 0.001,
                    "min_silence_duration_ms": 10,
                    "min_speech_duration_ms": 10,
                    "speech_pad_ms": 5
                }
            ]
            for params in fallback_params:
                logger.info(f"ä½¿ç”¨å›é€€VADå‚æ•°: {params}")
                speech_timestamps = self.vad_get_speech_timestamps(
                    audio_tensor,
                    self.vad_model,
                    sampling_rate=self.SAMPLE_RATE,
                    **params,
                    return_seconds=False
                )
                if speech_timestamps:
                    break
        # å¼ºåˆ¶åˆ†æ®µ
        max_segment_duration = 30 * self.SAMPLE_RATE  # 30ç§’
        final_timestamps = []
        for segment in speech_timestamps:
            start = segment['start']
            end = segment['end']
            segment_duration = end - start
            if segment_duration > max_segment_duration:
                num_splits = int(np.ceil(segment_duration / max_segment_duration))
                split_duration = segment_duration // num_splits
                for i in range(num_splits):
                    split_start = start + i * split_duration
                    split_end = min(split_start + split_duration, end)
                    final_timestamps.append({'start': split_start, 'end': split_end})
            else:
                final_timestamps.append(segment)
        logger.info(f"VAD æ£€æµ‹åˆ° {len(final_timestamps)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
        return final_timestamps

    def optimize_generation_parameters(self, audio_duration, audio_quality=0.5, language=None):
        """ä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼Œæ”¯æŒæ™ºèƒ½å®¢æœåœºæ™¯"""
        base_params = {
            "max_length": int(audio_duration * 60),
            "num_beams": 12,
            "temperature": 0.2,
            "no_repeat_ngram_size": 3,
            "early_stopping": False,
            "repetition_penalty": 1.2
        }
        if language == "yue":
            base_params.update({
                "temperature": 0.15,
                "num_beams": 12,
                "repetition_penalty": 1.3,
                "no_repeat_ngram_size": 4
            })
        elif language == "en":
            base_params.update({
                "temperature": 0.15,
                "num_beams": 12,
                "repetition_penalty": 1.25,
                "no_repeat_ngram_size": 3
            })
        if audio_duration > 60:
            base_params.update({
                "max_length": int(audio_duration * 80),
                "num_beams": 8
            })
        elif audio_duration < 5:
            base_params.update({
                "max_length": 256,
                "num_beams": 6,
                "early_stopping": True
            })
        if audio_quality < 0.4:
            base_params.update({
                "num_beams": 14,
                "repetition_penalty": 1.5
            })
        return base_params

    def clean_text(self, text, language=None):
        """æ¸…ç†è½¬å½•æ–‡æœ¬ï¼Œæ”¯æŒç²¤è¯­å’Œè‹±æ–‡"""
        if not text or not isinstance(text, str):
            return ""
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\-()ï¼ˆï¼‰]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        text = re.sub(r'([.!?ã€‚ï¼ï¼Ÿ])([^\s])', r'\1 \2', text)
        # ä¿®å¤å¸¸è§é”™è¯¯
        text = re.sub(r'K2670666\b', 'K26706656', text)
        if language == "yue":
            text = self.postprocess_cantonese(text)
        elif language == "en":
            text = self.postprocess_english(text)
        return text

    def postprocess_cantonese(self, text):
        """ç²¤è¯­è½¬å½•åå¤„ç†"""
        if not text:
            return text
        for wrong, correct in self.cantonese_corrections.items():
            text = text.replace(wrong, correct)
        text = re.sub(r'(å””)([^ä¿‚è¦å¥½éŒ¯])', r'\1 \2', text)
        text = re.sub(r'(å˜…)([^\.ã€‚!ï¼?ï¼Ÿ,ï¼Œ])', r'\1 \2', text)
        return text

    def postprocess_english(self, text):
        """è‹±æ–‡è½¬å½•åå¤„ç†"""
        if not text:
            return text
        for wrong, correct in self.english_corrections.items():
            text = text.replace(wrong, correct)
        text = re.sub(r'\b(\w+)\s+\1\b', r'\1', text)
        text = text[0].upper() + text[1:] if text else text
        return text

    def transcribe_cantonese_optimized(self, audio_np, audio_duration, audio_quality=0.5):
        """ç²¤è¯­ä¼˜åŒ–è½¬å½•"""
        logger.info("ä½¿ç”¨ç²¤è¯­ä¼˜åŒ–è½¬å½•æ¨¡å¼")
        cantonese_params = {
            "max_length": int(audio_duration * 60),
            "num_beams": 12,
            "temperature": 0.15,
            "no_repeat_ngram_size": 4,
            "early_stopping": False,
            "repetition_penalty": 1.3,
            "language": "yue",
            "task": "transcribe"
        }
        inputs = self.processor(
            audio_np,
            sampling_rate=self.SAMPLE_RATE,
            return_tensors="pt"
        )
        input_features = inputs["input_features"].to(self.device)
        model_dtype = next(self.whisper_model.parameters()).dtype
        input_features = input_features.to(model_dtype)
        with torch.no_grad():
            generated_ids = self.whisper_model.generate(
                input_features,
                **cantonese_params
            )
        transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        final_transcription = self.clean_text(transcription, "yue")
        logger.info(f"ç²¤è¯­ä¼˜åŒ–è½¬å½•ç»“æœ: {final_transcription}")
        return final_transcription

    def transcribe_english_optimized(self, audio_np, audio_duration, audio_quality=0.5):
        """è‹±æ–‡ä¼˜åŒ–è½¬å½•"""
        logger.info("ä½¿ç”¨è‹±æ–‡ä¼˜åŒ–è½¬å½•æ¨¡å¼")
        english_params = {
            "max_length": int(audio_duration * 60),
            "num_beams": 12,
            "temperature": 0.15,
            "no_repeat_ngram_size": 3,
            "early_stopping": False,
            "repetition_penalty": 1.25,
            "language": "en",
            "task": "transcribe"
        }
        inputs = self.processor(
            audio_np,
            sampling_rate=self.SAMPLE_RATE,
            return_tensors="pt"
        )
        input_features = inputs["input_features"].to(self.device)
        model_dtype = next(self.whisper_model.parameters()).dtype
        input_features = input_features.to(model_dtype)
        with torch.no_grad():
            generated_ids = self.whisper_model.generate(
                input_features,
                **english_params
            )
        transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        final_transcription = self.clean_text(transcription, "en")
        logger.info(f"è‹±æ–‡ä¼˜åŒ–è½¬å½•ç»“æœ: {final_transcription}")
        return final_transcription

    def transcribe_mixed_audio(self, audio_np, audio_duration, audio_quality=0.5):
        """å¤„ç†ç²¤è¯­+è‹±æ–‡æ··åˆéŸ³é¢‘ï¼Œç§»é™¤è¯­è¨€æ ‡ç­¾"""
        logger.info("å¤„ç†ç²¤è¯­+è‹±æ–‡æ··åˆéŸ³é¢‘ï¼Œç§»é™¤è¯­è¨€æ ‡ç­¾")
        audio_tensor = torch.from_numpy(audio_np).float().to("cpu")
        if len(audio_tensor.shape) > 1:
            audio_tensor = torch.mean(audio_tensor, dim=1)
        speech_timestamps = self.adaptive_vad_processing(audio_tensor)
        if not speech_timestamps:
            logger.warning("VAD æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œå°è¯•å®Œæ•´éŸ³é¢‘è½¬å½•")
            detected_language = self.detect_language(audio_np)
            if detected_language == "yue":
                return self.transcribe_cantonese_optimized(audio_np, audio_duration, audio_quality), detected_language
            elif detected_language == "en":
                return self.transcribe_english_optimized(audio_np, audio_duration, audio_quality), detected_language
            else:
                return self.transcribe_full_audio(audio_np, audio_duration, detected_language, audio_quality), detected_language
        all_transcriptions = []
        for i, segment_info in enumerate(speech_timestamps):
            start_sample = segment_info['start']
            end_sample = segment_info['end']
            segment = audio_np[start_sample:end_sample]
            segment_duration = (end_sample - start_sample) / self.SAMPLE_RATE
            logger.info(f"å¤„ç†ç‰‡æ®µ {i+1}/{len(speech_timestamps)}: æ—¶é•¿: {segment_duration:.2f}ç§’")
            segment_language = self.detect_language(segment, segment_duration=min(1.0, segment_duration))
            if segment_language == "yue":
                transcription = self.transcribe_cantonese_optimized(segment, segment_duration, audio_quality)
            elif segment_language == "en":
                transcription = self.transcribe_english_optimized(segment, segment_duration, audio_quality)
            else:
                transcription = self.transcribe_full_audio(segment, segment_duration, segment_language, audio_quality)
            if transcription:
                all_transcriptions.append(transcription)
                logger.info(f"ç‰‡æ®µ {i+1} è½¬å½•ç»“æœ: {transcription} ({segment_language})")
            torch.cuda.empty_cache()
        final_transcription = " ".join(all_transcriptions)
        logger.info("å·²ç§»é™¤è¯­è¨€æ ‡ç­¾")
        return final_transcription, "mixed"

    def transcribe_full_audio(self, audio_np, audio_duration, detected_language=None, audio_quality=0.5):
        """ç›´æ¥å¤„ç†å®Œæ•´éŸ³é¢‘"""
        if detected_language == "yue":
            return self.transcribe_cantonese_optimized(audio_np, audio_duration, audio_quality)
        elif detected_language == "en":
            return self.transcribe_english_optimized(audio_np, audio_duration, audio_quality)
        logger.info("ğŸ”„ ç›´æ¥å¤„ç†å®Œæ•´éŸ³é¢‘...")
        # è½¬æ¢è¯­è¨€ä»£ç 
        if detected_language:
            detected_language = self.language_mapping.get(detected_language, "yue")
        if detected_language not in self.supported_languages:
            logger.warning(f"æ— æ•ˆè¯­è¨€ä»£ç : {detected_language}ï¼Œå›é€€åˆ°ç²¤è¯­")
            detected_language = "yue"
        # åˆ†å—å¤„ç†é•¿éŸ³é¢‘
        chunk_duration = 30  # æ¯å—30ç§’
        num_chunks = int(np.ceil(audio_duration / chunk_duration))
        all_transcriptions = []
        for i in range(num_chunks):
            start_sample = int(i * chunk_duration * self.SAMPLE_RATE)
            end_sample = min(int((i + 1) * chunk_duration * self.SAMPLE_RATE), len(audio_np))
            chunk = audio_np[start_sample:end_sample]
            chunk_duration = (end_sample - start_sample) / self.SAMPLE_RATE
            logger.info(f"å¤„ç†éŸ³é¢‘å— {i+1}/{num_chunks}: æ—¶é•¿: {chunk_duration:.2f}ç§’")
            gen_params = self.optimize_generation_parameters(chunk_duration, audio_quality, detected_language)
            inputs = self.processor(
                chunk,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt"
            )
            input_features = inputs["input_features"].to(self.device)
            model_dtype = next(self.whisper_model.parameters()).dtype
            input_features = input_features.to(model_dtype)
            with torch.no_grad():
                generate_kwargs = gen_params.copy()
                if detected_language:
                    generate_kwargs["language"] = detected_language
                try:
                    generated_ids = self.whisper_model.generate(
                        input_features,
                        task="transcribe",
                        **generate_kwargs
                    )
                except ValueError as e:
                    logger.warning(f"ç”Ÿæˆå¤±è´¥: {e}ï¼Œå°è¯•æ— è¯­è¨€æŒ‡å®šè½¬å½•")
                    generate_kwargs.pop("language", None)
                    generated_ids = self.whisper_model.generate(
                        input_features,
                        task="transcribe",
                        **generate_kwargs
                    )
            transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            final_transcription = self.clean_text(transcription, detected_language)
            if final_transcription:
                all_transcriptions.append(final_transcription)
            torch.cuda.empty_cache()
        final_transcription = " ".join(all_transcriptions)
        logger.info(f"å®Œæ•´éŸ³é¢‘è½¬å½•ç»“æœ: {final_transcription}")
        return final_transcription

    def transcribe_audio(self, audio_bytes, forced_language=None):
        """è½¬å½•å®Œæ•´éŸ³é¢‘ï¼Œæ”¯æŒå¼ºåˆ¶è¯­è¨€è®¾ç½®"""
        if not self.model_loaded:
            logger.error("æ¨¡å‹æœªåŠ è½½")
            return "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€", "unknown"
        try:
            start_time = time.time()
            audio_np = self.process_audio(audio_bytes)
            audio_np = self.enhance_telephone_audio(audio_np)
            audio_duration = len(audio_np) / self.SAMPLE_RATE
            audio_quality = self.estimate_audio_quality(audio_np)
            logger.info(f"å¢å¼ºåéŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’, è´¨é‡: {audio_quality:.2f}")
            if forced_language:
                detected_language = self.language_mapping.get(forced_language, forced_language)
                logger.info(f"ä½¿ç”¨å¼ºåˆ¶è¯­è¨€: {detected_language}")
            else:
                detected_language = self.detect_language(audio_np)
            if detected_language in ["yue", "en"]:
                transcription, final_language = self.transcribe_mixed_audio(audio_np, audio_duration, audio_quality)
            else:
                transcription = self.transcribe_full_audio(audio_np, audio_duration, detected_language, audio_quality)
                final_language = detected_language or "unknown"
            logger.info(f"è½¬å½•æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
            return transcription if transcription else "æœªæ£€æµ‹åˆ°è¯­éŸ³", final_language
        except Exception as e:
            logger.error(f"è½¬å½•å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return f"è½¬å½•å¤±è´¥: {str(e)}", "error"

# åˆ›å»ºå…¨å±€å®ä¾‹
whisperAsr = WhisperASR()

@asynccontextmanager
async def lifespan(app):
    """FastAPI ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡ï¼ŒåŠ è½½æ¨¡å‹ä¸­...")
        whisperAsr.load_models(MODEL_PATH)
        logger.info("âœ… æœåŠ¡å¯åŠ¨å®Œæˆ")
        yield
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise
    finally:
        logger.info("ğŸ›‘ æœåŠ¡å…³é—­ï¼Œæ¸…ç†èµ„æº...")
        whisperAsr.clear_gpu_memory()

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Whisperè¯­éŸ³è¯†åˆ«API",
    description="åŸºäºWhisperçš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œä¼˜åŒ–æ™ºèƒ½å®¢æœåœºæ™¯ï¼ˆç²¤è¯­+è‹±æ–‡ï¼Œå«å£éŸ³å’Œå™ªéŸ³ï¼‰",
    version="2.7.1",
    lifespan=lifespan
)

@app.post("/transcribe", response_model=AudioResponse)
async def transcribe_audio(
    file: UploadFile = File(...),
    forced_language: Optional[str] = Query(None, description="å¼ºåˆ¶æŒ‡å®šè¯­è¨€ï¼Œå¦‚ï¼šcantonese, english")
):
    """éŸ³é¢‘è½¬å½•æ¥å£ï¼Œæ”¯æŒå¼ºåˆ¶è¯­è¨€è®¾ç½®"""
    try:
        if not whisperAsr.model_loaded:
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨")
        start_time = time.time()
        if not file.filename.lower().endswith(('.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac')):
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        audio_bytes = await file.read()
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
        logger.info(f"æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶: {file.filename}, å¤§å°: {len(audio_bytes)} bytes")
        loop = asyncio.get_event_loop()
        transcription, detected_language = await loop.run_in_executor(
            whisperAsr.executor, whisperAsr.transcribe_audio, audio_bytes, forced_language
        )
        total_time = time.time() - start_time
        logger.info(f"è¯·æ±‚å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’")
        return AudioResponse(
            transcription=transcription, 
            language="auto",
            detected_language=detected_language
        )
    except HTTPException:
        raise
    except torch.cuda.OutOfMemoryError:
        whisperAsr.clear_gpu_memory()
        raise HTTPException(status_code=500, detail="GPUå†…å­˜ä¸è¶³")
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Whisperå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æœåŠ¡ - æ™ºèƒ½å®¢æœä¼˜åŒ–ç‰ˆ",
        "status": "è¿è¡Œä¸­" if whisperAsr.model_loaded else "å¯åŠ¨ä¸­",
        "supported_languages": ["auto", "cantonese", "english", "chinese", "japanese", "korean", "vietnamese"],
        "endpoints": {
            "transcribe": "POST /transcribe - ä¸Šä¼ éŸ³é¢‘è¿›è¡Œè½¬å½•",
            "health": "GET /health - æœåŠ¡å¥åº·æ£€æŸ¥",
            "info": "GET /info - æ¨¡å‹ä¿¡æ¯"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy" if whisperAsr.model_loaded else "unhealthy",
        "model_loaded": whisperAsr.model_loaded,
        "gpu_available": torch.cuda.is_available(),
        "timestamp": time.time()
    }

@app.get("/info")
async def model_info():
    """æ¨¡å‹ä¿¡æ¯æ¥å£"""
    if not whisperAsr.model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    return {
        "model_name": "Whisper-large-v3",
        "language": "å¤šè¯­è¨€è‡ªåŠ¨æ£€æµ‹ï¼Œä¼˜åŒ–ç²¤è¯­å’Œè‹±æ–‡ï¼ˆæ™ºèƒ½å®¢æœåœºæ™¯ï¼‰",
        "special_optimization": "ç²¤è¯­+è‹±æ–‡æ··åˆï¼Œå£éŸ³å’Œå™ªéŸ³å¤„ç†ï¼Œé•¿éŸ³é¢‘åˆ†å—ï¼Œæ— è¯­è¨€æ ‡ç­¾è¾“å‡º",
        "device": str(whisperAsr.device),
        "model_loaded": True,
        "optimizations": ["n_mels=256", "beam_size=10", "temperature=0.2", "åŠ¨æ€é™å™ª", "è‡ªé€‚åº”VAD", "ç²¤è¯­ä¼˜åŒ–", "è‹±æ–‡ä¼˜åŒ–", "æ··åˆè¯­è¨€å¤„ç†", "é•¿éŸ³é¢‘åˆ†å—", "æ— è¯­è¨€æ ‡ç­¾"]
    }

if __name__ == "__main__":
    logger.info("Starting Whisper ASR Server with Cantonese+English optimization for customer service...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        timeout_keep_alive=300,
        log_level="info"
    )