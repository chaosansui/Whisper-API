import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
import logging
import asyncio
import uvicorn
import traceback
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from contextlib import asynccontextmanager
from pydantic import BaseModel
from typing import Optional
import time
from whisper_asr import WhisperASR
from config import API_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# å®šä¹‰å“åº”æ¨¡å‹
class AudioResponse(BaseModel):
    transcription: str
    language: str = "auto"
    detected_language: str = "unknown"

class CorrectionRequest(BaseModel):
    language: str
    wrong: str
    correct: str

class BatchCorrectionRequest(BaseModel):
    language: str
    corrections: dict

# åˆ›å»ºå…¨å±€å®ä¾‹
whisperAsr = WhisperASR()

@asynccontextmanager
async def lifespan(app):
    """FastAPI ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡ï¼ŒåŠ è½½æ¨¡å‹ä¸­...")
        whisperAsr.load_models()
        logger.info("âœ… æœåŠ¡å¯åŠ¨å®Œæˆ")
        yield
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise
    finally:
        logger.info("ğŸ›‘ æœåŠ¡å…³é—­ï¼Œæ¸…ç†èµ„æº...")
        whisperAsr.clear_gpu_memory()

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Whisperè¯­éŸ³è¯†åˆ«API",
    description="åŸºäºWhisperçš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œä¼˜åŒ–æ™ºèƒ½å®¢æœåœºæ™¯ï¼ˆç²¤è¯­+è‹±æ–‡ï¼Œå«å£éŸ³å’Œå™ªéŸ³ï¼‰",
    version="2.7.1",
    lifespan=lifespan
)

@app.post("/transcribe", response_model=AudioResponse)
async def transcribe_audio(
    file: UploadFile = File(...),
    forced_language: Optional[str] = Query(None, description="å¼ºåˆ¶æŒ‡å®šè¯­è¨€ï¼Œå¦‚ï¼šcantonese, english")
):
    """éŸ³é¢‘è½¬å½•æ¥å£ï¼Œæ”¯æŒå¼ºåˆ¶è¯­è¨€è®¾ç½®"""
    try:
        if not whisperAsr.model_loaded:
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨")
        
        start_time = time.time()
        if not file.filename.lower().endswith(('.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac')):
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        
        audio_bytes = await file.read()
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶: {file.filename}, å¤§å°: {len(audio_bytes)} bytes")
        
        loop = asyncio.get_event_loop()
        transcription, detected_language = await loop.run_in_executor(
            whisperAsr.executor, whisperAsr.transcribe_audio, audio_bytes, forced_language
        )
        
        total_time = time.time() - start_time
        logger.info(f"è¯·æ±‚å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        return AudioResponse(
            transcription=transcription, 
            language="auto",
            detected_language=detected_language
        )
    except HTTPException:
        raise
    except torch.cuda.OutOfMemoryError:
        whisperAsr.clear_gpu_memory()
        raise HTTPException(status_code=500, detail="GPUå†…å­˜ä¸è¶³")
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/vocabulary/add")
async def add_correction(correction: CorrectionRequest):
    """æ·»åŠ è‡ªå®šä¹‰è¯æ±‡ä¿®æ­£"""
    try:
        whisperAsr.add_custom_correction(
            correction.language,
            correction.wrong,
            correction.correct
        )
        return {"message": "ä¿®æ­£æ·»åŠ æˆåŠŸ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ·»åŠ ä¿®æ­£å¤±è´¥: {str(e)}")

@app.post("/vocabulary/batch_add")
async def batch_add_corrections(batch_correction: BatchCorrectionRequest):
    """æ‰¹é‡æ·»åŠ è¯æ±‡ä¿®æ­£"""
    try:
        whisperAsr.batch_add_corrections(
            batch_correction.language,
            batch_correction.corrections
        )
        return {"message": f"æ‰¹é‡æ·»åŠ  {len(batch_correction.corrections)} ä¸ªä¿®æ­£æˆåŠŸ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ·»åŠ ä¿®æ­£å¤±è´¥: {str(e)}")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Whisperå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æœåŠ¡ - æ™ºèƒ½å®¢æœä¼˜åŒ–ç‰ˆ",
        "status": "è¿è¡Œä¸­" if whisperAsr.model_loaded else "å¯åŠ¨ä¸­",
        "supported_languages": ["auto", "cantonese", "english", "chinese", "japanese", "korean", "vietnamese"],
        "endpoints": {
            "transcribe": "POST /transcribe - ä¸Šä¼ éŸ³é¢‘è¿›è¡Œè½¬å½•",
            "health": "GET /health - æœåŠ¡å¥åº·æ£€æŸ¥",
            "info": "GET /info - æ¨¡å‹ä¿¡æ¯",
            "vocabulary_add": "POST /vocabulary/add - æ·»åŠ è¯æ±‡ä¿®æ­£",
            "vocabulary_batch_add": "POST /vocabulary/batch_add - æ‰¹é‡æ·»åŠ è¯æ±‡ä¿®æ­£"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy" if whisperAsr.model_loaded else "unhealthy",
        "model_loaded": whisperAsr.model_loaded,
        "gpu_available": torch.cuda.is_available(),
        "timestamp": time.time()
    }

@app.get("/info")
async def model_info():
    """æ¨¡å‹ä¿¡æ¯æ¥å£"""
    if not whisperAsr.model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    return {
        "model_name": "Whisper-large-v3",
        "language": "å¤šè¯­è¨€è‡ªåŠ¨æ£€æµ‹ï¼Œä¼˜åŒ–ç²¤è¯­å’Œè‹±æ–‡ï¼ˆæ™ºèƒ½å®¢æœåœºæ™¯ï¼‰",
        "special_optimization": "ç²¤è¯­+è‹±æ–‡æ··åˆï¼Œå£éŸ³å’Œå™ªéŸ³å¤„ç†ï¼Œé•¿éŸ³é¢‘åˆ†å—ï¼Œæ— è¯­è¨€æ ‡ç­¾è¾“å‡º",
        "device": str(whisperAsr.device),
        "model_loaded": True,
        "optimizations": ["n_mels=256", "beam_size=10", "temperature=0.2", "åŠ¨æ€é™å™ª", "è‡ªé€‚åº”VAD", "ç²¤è¯­ä¼˜åŒ–", "è‹±æ–‡ä¼˜åŒ–", "æ··åˆè¯­è¨€å¤„ç†", "é•¿éŸ³é¢‘åˆ†å—", "æ— è¯­è¨€æ ‡ç­¾"]
    }

if __name__ == "__main__":
    logger.info("Starting Whisper ASR Server with Cantonese+English optimization for customer service...")
    uvicorn.run(
        app,
        host=API_CONFIG["host"],
        port=API_CONFIG["port"],
        timeout_keep_alive=API_CONFIG["timeout_keep_alive"],
        log_level=API_CONFIG["log_level"]
    )