import torch
import numpy as np
import traceback
import re
import logging
import os
import time
import torchaudio
import math # å¯¼å…¥ math åº“ç”¨äº RMS è®¡ç®—
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from preprocessing.audio_enhancer import AudioEnhancer
from preprocessing.vad_segmenter import VADSegmenter, VAD_SAMPLE_RATE
from preprocessing.diarization_core import DiarizationCore 
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, List, Dict, Tuple
from scipy.signal import butter, filtfilt
import io
from config import MODEL_PATH, SAMPLE_RATE, SPEAKER_EMBEDDING_PATH

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s",
)
logger = logging.getLogger(__name__)

CONFIDENCE_THRESHOLD = -0.7 

RMS_ENERGY_THRESHOLD = 0.015 

def process_audio(audio_bytes: bytes) -> np.ndarray:
    """å°†éŸ³é¢‘å­—èŠ‚æµè½¬æ¢ä¸ºå•å£°é“ 16kHz NumPy æ•°ç»„"""
    try:
        waveform, sample_rate = torchaudio.load(io.BytesIO(audio_bytes))
        if waveform.shape[0] > 1:
            # è½¬æ¢ä¸ºå•å£°é“
            waveform = waveform.mean(dim=0, keepdim=True)
        if sample_rate != SAMPLE_RATE:
            # é‡é‡‡æ ·è‡³ç›®æ ‡é‡‡æ ·ç‡
            resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE)
            waveform = resampler(waveform)
        audio_np = waveform.squeeze().numpy()
        if np.max(np.abs(audio_np)) < 1e-3:
            logger.warning("éŸ³é¢‘å¹…åº¦è¿‡ä½ï¼Œå¯èƒ½ä¸ºé™éŸ³")
        return audio_np
    except Exception as e:
        logger.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
        raise

def calculate_rms_energy(audio_segment: np.ndarray) -> float:
    """è®¡ç®—éŸ³é¢‘ç‰‡æ®µçš„ RMS (Root Mean Square) èƒ½é‡ã€‚"""
    if len(audio_segment) == 0:
        return 0.0
    # RMS = sqrt(mean(x^2))
    return np.sqrt(np.mean(audio_segment**2))


class WhisperASR:
    def __init__(self, hf_token: Optional[str] = None):
        if not torch.cuda.is_available():
            logger.error("CUDA ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ CPU")
            self.device = torch.device("cpu")
        else:
            self.device = torch.device("cuda:0") 
            torch.cuda.set_device(0)
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}, GPU: {torch.cuda.get_device_name(0)}")
        
        self.torch = torch
        self.whisper_model = None
        self.processor = None
        self.enhancer = None
        self.vad_segmenter = None
        self.diarization_core = None
        
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.model_loaded = False
        self.SAMPLE_RATE = SAMPLE_RATE
        self.hf_token = hf_token

        self.optimization_config = {
            "use_flash_attention": False,
            "chunk_length": 30,
            "batch_size": 1,
            "max_retries": 5,
            "timeout": 60.0,
            "n_mels": 128
        }
        
        # ç§»é™¤ language_mappingï¼Œä»…ä¿ç•™æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        self.supported_languages = ['en', 'zh', 'yue', 'auto'] 

    def log_memory(self, stage: str):
        """è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == "cuda":
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            free_memory = total_memory - allocated_memory
            logger.debug(f"{stage}: å¯ç”¨ {free_memory:.2f} GiB / æ€»å…± {total_memory:.2f} GiB")

    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            self.log_memory("æ¸…ç†åå†…å­˜")

    def load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        try:
            self.clear_gpu_memory()
            logger.debug(f"ğŸ”„ å°è¯•åŠ è½½ Whisper æ¨¡å‹: {MODEL_PATH}")
            
            # åŠ è½½ Whisper æ¨¡å‹
            self.processor = WhisperProcessor.from_pretrained(MODEL_PATH)
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                MODEL_PATH,
                torch_dtype=torch.float16,
                device_map="auto"
            ).to(self.device)
            self.whisper_model.eval()
            logger.info("âœ… Whisper æ¨¡å‹åŠ è½½æˆåŠŸ")

            # ã€åŠ è½½æ¨¡å—åŒ–å‰å¤„ç†æ¨¡å‹ã€‘
            logger.debug("ğŸ”„ å°è¯•åŠ è½½æ¨¡å—åŒ–çš„ Diarization ä¾èµ–...")
            
            # 1. åˆå§‹åŒ– Enhancer (æµç¨‹ 1, 2, 4)
            self.enhancer = AudioEnhancer(sample_rate=self.SAMPLE_RATE)
            
            # 2. åˆå§‹åŒ– VAD Segmenter (æµç¨‹ 3) - VAD åœ¨ CPU ä¸Šè¿è¡Œï¼Œä½¿ç”¨ 16kHz
            self.vad_segmenter = VADSegmenter(sample_rate=VAD_SAMPLE_RATE, device='cpu')
            
            # 3. åˆå§‹åŒ– Diarization Core (æµç¨‹ 5, 8, 9) - è¿è¡Œåœ¨ GPU ä¸Š
            self.diarization_core = DiarizationCore(
                speaker_embedding_path=SPEAKER_EMBEDDING_PATH,
                sample_rate=self.SAMPLE_RATE,
                device=self.device
            )

            logger.info("âœ… æ¨¡å—åŒ– Diarization ä¾èµ–åŠ è½½å®Œæˆ")
            self.model_loaded = True
            logger.info("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
            self.warmup_model()

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            logger.error(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
            self.model_loaded = False
            raise

    def warmup_model(self):
        """é¢„çƒ­æ¨¡å‹"""
        logger.debug("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        try:
            t = np.linspace(0, 1, self.SAMPLE_RATE).astype(np.float32)
            dummy_audio = 0.1 * np.sin(2 * np.pi * 300 * t)
            inputs = self.processor(
                dummy_audio,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt",
                return_attention_mask=True
            )
            input_features = inputs["input_features"].to(self.device, dtype=torch.float16)
            attention_mask = inputs.get("attention_mask").to(self.device) if inputs.get("attention_mask") is not None else None
            
            with torch.no_grad():
                for lang in ["yue", "en"]:
                    _ = self.whisper_model.generate(
                        input_features,
                        attention_mask=attention_mask,
                        task="transcribe",
                        max_length=10,
                        num_beams=1,
                        language=lang
                    )
            
            logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}, ç»§ç»­å¯åŠ¨")

    def detect_language(self, audio_np: np.ndarray, segment_duration: float = 5.0) -> str:
        """æ£€æµ‹éŸ³é¢‘è¯­è¨€ï¼Œé€‚é… whisper-large-v3ã€‚
        ç”±äºæ¨¡å‹çš„è¯­è¨€æ£€æµ‹èƒ½åŠ›ï¼Œæ­¤æ–¹æ³•æš‚è¢« stub æ›¿ä»£ã€‚"""
        logger.warning("è¯­è¨€æ£€æµ‹æš‚æ—¶è·³è¿‡ï¼Œå›é€€åˆ°ç²¤è¯­ (yue) æˆ–ä½¿ç”¨ 'auto'")
        return "yue" 

    def map_speaker_to_role(self, text: str) -> str:
        """æ ¹æ®æ–‡æœ¬å†…å®¹åˆ¤æ–­è¯´è¯äººè§’è‰²"""
        agent_keywords = ["ä½ å¥½", "æ¬¢è¿è‡´ç”µ", "å®¢æœ", "æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©", "hello", "hi", "welcome"]
        return "å®¢æœ" if any(keyword in text.lower() for keyword in agent_keywords) else "å®¢æˆ·"

    def _post_process_text(self, text: str, language: str) -> str:
        """
        æµç¨‹ 10: æ¸…ç†è½¬å½•æ–‡æœ¬ï¼Œå»é™¤åœé¡¿è¯ã€é‡å¤æ ‡ç‚¹ã€ä¿®æ­£å¥æœ«æ ‡ç‚¹ç­‰ã€‚
        """
        if not text:
            return ""

        # 1. ç§»é™¤å¸¸è§çš„æ‚éŸ³æ ‡è®°å’Œåœé¡¿è¯
        text = re.sub(r'\[\s*(laughter|music|sigh|whispering|cough)\s*\]', '', text, flags=re.IGNORECASE)
        # ç§»é™¤å¸¸è§çš„â€œå‘ƒâ€æˆ–â€œå—¯â€ç­‰éè¯­ä¹‰åœé¡¿
        text = re.sub(r'\s*å‘ƒ(å‘ƒ)*\s*|\s*å—¯(å—¯)*\s*', ' ', text) 

        # 2. ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        # åˆå¹¶å¤šä¸ªç›¸åŒçš„æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'([ï¼Œã€‚ï¼ï¼Ÿ])\s*\1+', r'\1', text) 
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 3. ä¿®æ­£ Whisper æœ‰æ—¶åœ¨å¼€å¤´åŠ å…¥çš„é‡å¤çŸ­è¯­ï¼ˆä¾‹å¦‚ "ä½ å¥½ ä½ å¥½" -> "ä½ å¥½"ï¼‰
        words = text.split()
        if len(words) > 3 and words[0] == words[1] and words[0] != words[2]:
            text = ' '.join(words[1:])

        # 4. ç¡®ä¿ä¸­æ–‡æˆ–ç²¤è¯­å¥å°¾æœ‰æ ‡ç‚¹
        if language in ['zh', 'yue'] and text and text[-1] not in ['ã€‚', 'ï¼', 'ï¼Ÿ', '...']:
            text += 'ã€‚'
            
        return text

    def transcribe_segment(self, audio_np: np.ndarray, duration: float, language: str) -> Tuple[str, float]:
        """
        [æµç¨‹ 6] è½¬å½•å•ä¸ªéŸ³é¢‘ç‰‡æ®µã€‚
        è¿”å›å€¼: (transcription: str, confidence_score: float)
        """
        try:
            # === æ–°å¢ï¼šRMS èƒ½é‡è¿‡æ»¤æ›¿ä»£ç½®ä¿¡åº¦è¿‡æ»¤ ===
            rms_energy = calculate_rms_energy(audio_np)
            if rms_energy < RMS_ENERGY_THRESHOLD:
                logger.warning(f"éŸ³é¢‘èƒ½é‡è¿‡æ»¤ï¼šRMS {rms_energy:.4f} ä½äºé˜ˆå€¼ {RMS_ENERGY_THRESHOLD}ï¼Œè·³è¿‡è½¬å½•ã€‚")
                # è¿”å›ç©ºæ–‡æœ¬å’Œé«˜ç½®ä¿¡åº¦ (1.0)ï¼Œç¡®ä¿åœ¨åç»­æµç¨‹ä¸­å®ƒè¢«è§†ä¸ºæœ‰æ•ˆè¿‡æ»¤ï¼ˆå³ï¼Œæ— æ–‡æœ¬ï¼‰
                return "", 1.0 
            
            # ç§»é™¤æ—§çš„ã€ä¸ç²¾ç¡®çš„å¹…åº¦æ£€æŸ¥
            # if np.max(np.abs(audio_np)) < 1e-3:
            #     logger.warning("éŸ³é¢‘å¹…åº¦è¿‡ä½ï¼Œè·³è¿‡è½¬å½•")
            #     return "", 1.0 
            
            # ä½¿ç”¨ 'auto' è¯­è¨€é€‰é¡¹ï¼Œè®© Whisper è‡ªè¡Œæ£€æµ‹ï¼ˆé™¤éå¼ºåˆ¶æŒ‡å®šï¼‰
            if language == 'auto':
                lang_param = None
            else:
                lang_param = language

            gen_params = {
                "max_length": min(int(duration * 30), 200),
                "num_beams": 5,
                "temperature": 0.7,
                "no_repeat_ngram_size": 3,
                "repetition_penalty": 1.2,
                "language": lang_param, # ä½¿ç”¨å‚æ•°
                "task": "transcribe"
                # WARNING FIX: ç§»é™¤ output_scores=True å’Œ return_dict_in_generate=True
            }
            
            inputs = self.processor(
                audio_np,
                sampling_rate=self.SAMPLE_RATE,
                return_tensors="pt",
                return_attention_mask=True
            )
            input_features = inputs["input_features"].to(self.device, dtype=torch.float16)
            attention_mask = inputs.get("attention_mask").to(self.device) if inputs.get("attention_mask") is not None else None
            
            with torch.no_grad():
                # è­¦å‘Šä¿®å¤ï¼šä¸å†ä¼ å…¥ output_scores å’Œ return_dict_in_generate
                generated_ids = self.whisper_model.generate(
                    input_features,
                    attention_mask=attention_mask,
                    **gen_params
                )
            
            transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # WARNING FIX: æ— æ³•å¯é è·å–ç½®ä¿¡åº¦ï¼Œä¸´æ—¶è®¾ç½®ä¸º 1.0
            confidence_score = 1.0 
            logger.debug(f"â„¹ï¸ æµç¨‹ 7 (ç½®ä¿¡åº¦è¿‡æ»¤) å·²è¢« RMS èƒ½é‡è¿‡æ»¤æ›¿ä»£ã€‚è½¬å½•ç½®ä¿¡åº¦åˆ†æ•°è¢«è®¾ç½®ä¸º {confidence_score}ã€‚")
            
            return transcription, confidence_score
        
        except Exception as e:
            logger.warning(f"ç‰‡æ®µè½¬å½•å¤±è´¥: {e}")
            return "", -10.0 # å¤±è´¥æ—¶è¿”å›æä½çš„ç½®ä¿¡åº¦ï¼Œä½†æ­¤è·¯å¾„åº”è¯¥å¾ˆå°‘è§¦å‘

    def process_call_recording(self, audio_bytes: bytes, forced_language: Optional[str] = None) -> Tuple[List[Dict], str]:
        """å¤„ç†é€šè¯å½•éŸ³çš„ä¸»æ–¹æ³•"""
        try:
            start_time = time.time()
            
            # 1. åˆå§‹éŸ³é¢‘å¤„ç†
            audio_np = process_audio(audio_bytes)
            audio_duration = len(audio_np) / self.SAMPLE_RATE
            logger.info(f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’")

            # 2. è¯­è¨€æ£€æµ‹ (ç§»é™¤ language_mapping é€»è¾‘)
            if forced_language and forced_language in self.supported_languages:
                detected_language = forced_language
            else:
                # å¦‚æœæœªå¼ºåˆ¶æŒ‡å®šæˆ–æŒ‡å®šè¯­è¨€ä¸æ”¯æŒï¼Œåˆ™ä½¿ç”¨æ£€æµ‹ç»“æœ (ç›®å‰ stub ä¸º 'yue')
                detected_language = self.detect_language(audio_np)
            
            # ç¡®ä¿æœ€ç»ˆä½¿ç”¨ä¸€ä¸ªåˆç†çš„è¯­è¨€ä»£ç ï¼Œå¦åˆ™å›é€€åˆ° 'auto'
            if detected_language not in self.supported_languages:
                 detected_language = "auto"
                 logger.warning(f"æ— æ•ˆæˆ–æœªæ£€æµ‹åˆ°è¯­è¨€ï¼Œå°†ä½¿ç”¨ Whisper æ¨¡å‹çš„ 'auto' è¯­è¨€æ£€æµ‹ã€‚")

            # 3. è¿è¡Œæ¨¡å—åŒ– Diarization å’Œ Transcribe
            results = self._run_diarization_and_transcription(audio_np, audio_duration, detected_language)
            
            logger.info(f"å¤„ç†å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
            return results, detected_language
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            logger.error(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
            return [{
                "role": "é”™è¯¯",
                "speaker": "None",
                "text": f"å¤„ç†å¤±è´¥: {str(e)}",
                "start": 0.0,
                "end": 0.0
            }], "error"

    def _run_diarization_and_transcription(self, audio_np: np.ndarray, audio_duration: float, language: str) -> List[Dict]:
        """
        [æµç¨‹ 1-5, 8, 9] Diarization & [æµç¨‹ 6] Transcribe
        """
        results = []
        
        if not self.diarization_core or not self.vad_segmenter or not self.enhancer:
            logger.warning("âš ï¸ Diarization ä¾èµ–æœªåŠ è½½ï¼Œä½¿ç”¨å®Œæ•´è½¬å½•")
            return self._transcribe_full_audio(audio_np, audio_duration, language)
        
        if audio_duration < 2.0:
            logger.info("âš ï¸ éŸ³é¢‘è¿‡çŸ­ï¼Œä½¿ç”¨å®Œæ•´è½¬å½•")
            return self._transcribe_full_audio(audio_np, audio_duration, language)
        
        try:
            logger.info("ğŸ¯ å¼€å§‹æ¨¡å—åŒ–å‰å¤„ç†å’Œè½¬å½•...")
            
            # 1. è´¨é‡æ£€æµ‹ / é™å™ª / å¢å¼º (AudioEnhancer)
            self.enhancer.quality_check(audio_np) # æµç¨‹ 1
            enhanced_audio_np = self.enhancer.denoise_and_enhance(audio_np) # æµç¨‹ 2 & 4
            
            # 2. VAD åˆ†å‰² (VADSegmenter)
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿ VAD è¾“å…¥éŸ³é¢‘æ˜¯ 16kHz
            if self.SAMPLE_RATE != VAD_SAMPLE_RATE:
                logger.info(f"ğŸ”„ æ­£åœ¨å°†éŸ³é¢‘ä» {self.SAMPLE_RATE} Hz é‡é‡‡æ ·è‡³ VAD æ‰€éœ€çš„ {VAD_SAMPLE_RATE} Hz...")
                
                # è½¬æ¢ NumPy æ•°ç»„ä¸º Tensor
                audio_tensor = torch.from_numpy(enhanced_audio_np).float().unsqueeze(0)
                
                # æ‰§è¡Œé‡é‡‡æ ·
                resampler = torchaudio.transforms.Resample(self.SAMPLE_RATE, VAD_SAMPLE_RATE)
                # ä½¿ç”¨ to("cpu") ç¡®ä¿åœ¨ CPU ä¸Šè¿›è¡Œé‡é‡‡æ ·ï¼Œå‡å°‘ GPU è´Ÿæ‹…
                vad_input_tensor = resampler(audio_tensor.to("cpu")) 
                
                # è½¬æ¢å› NumPy æ•°ç»„
                vad_input_np = vad_input_tensor.squeeze().numpy()
            else:
                vad_input_np = enhanced_audio_np
            
            # æµç¨‹ 3
            # VADSegmenter ç°åœ¨æ¥æ”¶ 16kHz éŸ³é¢‘ï¼Œå¹¶ä¸”å…·æœ‰æ›´çŸ­çš„é™éŸ³é˜ˆå€¼ (åœ¨ vad_segmenter.py ä¸­å·²è®¾ç½®)
            self.vad_segmenter.sample_rate = VAD_SAMPLE_RATE # ä¸´æ—¶ä¿®æ­£ VADSegmenter å†…éƒ¨çš„é‡‡æ ·ç‡æ£€æŸ¥
            vad_segments = self.vad_segmenter.get_speech_segments(vad_input_np) 
            logger.info(f"ğŸ¯ VAD Segmenter å¾—åˆ° {len(vad_segments)} ä¸ªç‰‡æ®µ")

            # 3. è¯´è¯äººæ ¸å¿ƒåˆ†ç¦» (DiarizationCore)
            diarized_segments = self.diarization_core.diarize(enhanced_audio_np, vad_segments) # æµç¨‹ 5, 8, 9
            logger.info(f"ğŸ¯ Diarization Core å¾—åˆ° {len(diarized_segments)} ä¸ªå¸¦æ ‡ç­¾çš„ç‰‡æ®µ")

            if not diarized_segments:
                logger.warning("âŒ Diarization æ ¸å¿ƒæœªæ£€æµ‹åˆ°æœ‰æ•ˆç‰‡æ®µï¼Œå›é€€åˆ°å®Œæ•´è½¬å½•")
                return self._transcribe_full_audio(audio_np, audio_duration, language)

            # 4. è½¬å½•æ¯ä¸ªç‰‡æ®µ (æµç¨‹ 6)
            futures = []
            for segment in diarized_segments:
                # æäº¤ä»»åŠ¡ï¼š_transcribe_single_segment è¿”å› (transcription, confidence)
                futures.append(self.executor.submit(self._transcribe_single_segment, enhanced_audio_np, segment, language))
            
            successful_transcriptions = 0
            
            for i, future in enumerate(futures):
                transcription, confidence_score = future.result()
                
                # ã€æµç¨‹ 7: åˆ†æ®µè´¨é‡è¿‡æ»¤ - æ›¿ä»£æ–¹æ¡ˆï¼šæ£€æŸ¥è½¬å½•æ–‡æœ¬æ˜¯å¦ä¸ºç©ºã€‘
                # ç”±äº RMS è¿‡æ»¤ç°åœ¨åœ¨ transcribe_segment å†…éƒ¨æ‰§è¡Œï¼Œå¦‚æœè½¬å½•ç»“æœä¸ºç©ºï¼Œ
                # æ„å‘³ç€è¯¥ç‰‡æ®µå·²è¢« RMS è¿‡æ»¤æˆ–æ¨¡å‹è½¬å½•å¤±è´¥ã€‚
                if transcription and len(transcription.strip()) > 0:
                    
                    # ã€æµç¨‹ 10: æ–‡æœ¬åå¤„ç†ã€‘
                    processed_text = self._post_process_text(transcription, language)
                    
                    role = self.map_speaker_to_role(processed_text)
                    
                    # 5. ç»“æœèšåˆ
                    results.append({
                        "role": role,
                        "speaker": diarized_segments[i]["speaker"], 
                        "text": processed_text,
                        "start": diarized_segments[i]["start"],
                        "end": diarized_segments[i]["end"]
                    })
                    successful_transcriptions += 1
                else:
                    logger.warning(f"âŒ ç‰‡æ®µ {i+1} (è¯´è¯äºº: {diarized_segments[i]['speaker']}) è¢« RMS è¿‡æ»¤æˆ–è½¬å½•ä¸ºç©ºï¼Œå·²ä¸¢å¼ƒã€‚")
                self.clear_gpu_memory()
            
            if successful_transcriptions == 0:
                logger.warning("âŒ æ‰€æœ‰ç‰‡æ®µè½¬å½•å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´è½¬å½•")
                return self._transcribe_full_audio(audio_np, audio_duration, language)
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å—åŒ–è½¬å½•æµç¨‹å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
            return self._transcribe_full_audio(audio_np, audio_duration, language)
        
        return results

    def _transcribe_single_segment(self, audio_np: np.ndarray, segment: Dict, language: str) -> Tuple[Optional[str], float]:
        """è½¬å½•å•ä¸ªéŸ³é¢‘ç‰‡æ®µï¼Œå¹¶è¿”å› (è½¬å½•æ–‡æœ¬, ç½®ä¿¡åº¦)"""
        try:
            start = segment['start']
            end = segment['end']
            segment_duration = end - start
            
            if segment_duration < 0.1:
                return None, -10.0
            
            # ä½¿ç”¨å¢å¼ºåçš„éŸ³é¢‘è¿›è¡Œè½¬å½•
            audio_segment = audio_np[int(start * self.SAMPLE_RATE):int(end * self.SAMPLE_RATE)]
            if len(audio_segment) == 0:
                return None, -10.0
            
            transcription, confidence_score = self.transcribe_segment(audio_segment, segment_duration, language)
            self.clear_gpu_memory()
            return transcription, confidence_score
            
        except Exception as e:
            logger.warning(f"ç‰‡æ®µè½¬å½•å¤±è´¥: {e}")
            self.clear_gpu_memory()
            return None, -10.0

    def _transcribe_full_audio(self, audio_np: np.ndarray, audio_duration: float, language: str) -> List[Dict]:
        """å®Œæ•´éŸ³é¢‘è½¬å½•ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        transcription, _ = self.transcribe_segment(audio_np, audio_duration, language)
        return [{
            "role": "æœªçŸ¥",
            "speaker": "SPEAKER_00",
            "text": transcription,
            "start": 0.0,
            "end": audio_duration
        }]
